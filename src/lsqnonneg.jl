####
#### Unregularized NNLS problem
####

struct NNLSProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_work::W
end
function NNLSProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_work = NNLS.NNLSWorkspace(A, b)
    return NNLSProblem(A, b, m, n, nnls_work)
end

# Solve NNLS problem
solve!(work::NNLSProblem, A = work.A, b = work.b) = NNLS.nnls!(work.nnls_work, A, b)

@inline solution(work::NNLSProblem) = NNLS.solution(work.nnls_work)
@inline ncomponents(work::NNLSProblem) = NNLS.ncomponents(work.nnls_work)
@inline resnorm(work::NNLSProblem) = NNLS.residualnorm(work.nnls_work)
@inline resnorm_sq(work::NNLSProblem) = resnorm(work)^2

@doc raw"""
    lsqnonneg(A::AbstractMatrix, b::AbstractVector)

Compute the nonnegative least-squares (NNLS) solution ``X`` of the problem:

```math
X = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2.
```

# Arguments

  - `A::AbstractMatrix`: Left hand side matrix acting on `x`
  - `b::AbstractVector`: Right hand side vector

# Outputs

  - `X::AbstractVector`: NNLS solution
"""
lsqnonneg(A::AbstractMatrix, b::AbstractVector) = lsqnonneg!(lsqnonneg_work(A, b))
lsqnonneg_work(A::AbstractMatrix, b::AbstractVector) = NNLSProblem(A, b)
lsqnonneg!(work::NNLSProblem) = solve!(work)
lsqnonneg!(work::NNLSProblem{T}, A::AbstractMatrix{T}, b::AbstractVector{T}) where {T} = solve!(work, A, b)

####
#### Lazy wrappers for LHS matrix and RHS vector for augmented Tikhonov-regularized NNLS problems
####

struct PaddedVector{T, Tb <: AbstractVector{T}} <: AbstractVector{T}
    b::Tb
    pad::Int
end
Base.size(x::PaddedVector) = (length(x.b) + x.pad,)
Base.parent(x::PaddedVector) = x.b

function Base.copyto!(y::AbstractVector{T}, x::PaddedVector{T}) where {T}
    @assert size(x) == size(y)
    (; b, pad) = x
    m = length(b)
    @simd for i in 1:m
        y[i] = b[i]
    end
    @simd for i in m+1:m+pad
        y[i] = zero(T)
    end
    return y
end

struct TikhonovPaddedMatrix{T, TA <: AbstractMatrix{T}} <: AbstractMatrix{T}
    A::TA
    Œº::Base.RefValue{T}
end
TikhonovPaddedMatrix(A::AbstractMatrix, Œº::Real) = TikhonovPaddedMatrix(A, Ref(Œº))
Base.size(P::TikhonovPaddedMatrix) = ((m, n) = size(P.A); return (m + n, n))
Base.parent(P::TikhonovPaddedMatrix) = P.A
mu(P::TikhonovPaddedMatrix) = P.Œº[]
mu!(P::TikhonovPaddedMatrix, Œº::Real) = P.Œº[] = Œº

function Base.copyto!(B::AbstractMatrix{T}, P::TikhonovPaddedMatrix{T}) where {T}
    @assert size(P) == size(B)
    A, Œº = parent(P), mu(P)
    m, n = size(A)
    @inbounds for j in 1:n
        @simd for i in 1:m
            B[i, j] = A[i, j]
        end
        @simd for i in m+1:m+n
            B[i, j] = ifelse(i == m + j, Œº, zero(T))
        end
    end
    return B
end

####
#### Tikhonov regularized NNLS problem
####

struct NNLSTikhonovRegProblem{
    T,
    TA <: AbstractMatrix{T},
    Tb <: AbstractVector{T},
    W <: NNLSProblem{T, TikhonovPaddedMatrix{T, TA}, PaddedVector{T, Tb}},
    B,
}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W
    buffers::B
end
function NNLSTikhonovRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}, Œº::Real = T(NaN)) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(TikhonovPaddedMatrix(A, Œº), PaddedVector(b, n))
    buffers = (x = zeros(T, n), y = zeros(T, n))
    return NNLSTikhonovRegProblem(A, b, m, n, nnls_prob, buffers)
end

@doc raw"""
    lsqnonneg_tikh(A::AbstractMatrix, b::AbstractVector, Œº::Real)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||x||_2^2.
```

# Arguments

  - `A::AbstractMatrix`: Left hand side matrix acting on `x`
  - `b::AbstractVector`: Right hand side vector
  - `Œº::Real`: Regularization parameter

# Outputs

  - `X::AbstractVector`: NNLS solution
"""
lsqnonneg_tikh(A::AbstractMatrix, b::AbstractVector, Œº::Real) = lsqnonneg_tikh!(lsqnonneg_tikh_work(A, b), Œº)
lsqnonneg_tikh_work(A::AbstractMatrix, b::AbstractVector) = NNLSTikhonovRegProblem(A, b)
lsqnonneg_tikh!(work::NNLSTikhonovRegProblem, Œº::Real) = solve!(work, Œº)

mu(work::NNLSTikhonovRegProblem) = mu(work.nnls_prob.A)
mu!(work::NNLSTikhonovRegProblem, Œº::Real) = mu!(work.nnls_prob.A, Œº)

function solve!(work::NNLSTikhonovRegProblem, Œº::Real)
    # Set regularization parameter and solve NNLS problem
    mu!(work, Œº)
    solve!(work.nnls_prob)
    return solution(work)
end

@inline solution(work::NNLSTikhonovRegProblem) = NNLS.solution(work.nnls_prob.nnls_work)
@inline ncomponents(work::NNLSTikhonovRegProblem) = NNLS.ncomponents(work.nnls_prob.nnls_work)

@inline loss(work::NNLSTikhonovRegProblem) = NNLS.residualnorm(work.nnls_prob.nnls_work)^2

regnorm(work::NNLSTikhonovRegProblem) = mu(work)^2 * seminorm_sq(work) # Œº¬≤||x||¬≤
‚àáregnorm(work::NNLSTikhonovRegProblem) = 2 * mu(work) * seminorm_sq(work) + mu(work)^2 * ‚àáseminorm_sq(work) # d/dŒº [Œº¬≤||x||¬≤] = 2Œº||x||¬≤ + Œº¬≤ d/dŒº [||x||¬≤]

resnorm(work::NNLSTikhonovRegProblem) = ‚àö(resnorm_sq(work)) # ||Ax-b||
resnorm_sq(work::NNLSTikhonovRegProblem) = max(loss(work) - regnorm(work), 0) # ||Ax-b||¬≤
‚àáresnorm_sq(work::NNLSTikhonovRegProblem, ‚àá = gradient_temps(work)) = 4 * ‚àá.Œº^3 * ‚àá.x·µÄB‚Åª¬πx # d/dŒº [||Ax-b||¬≤]
‚àá¬≤resnorm_sq(work::NNLSTikhonovRegProblem, ‚àá¬≤ = hessian_temps(work)) = 12 * ‚àá¬≤.Œº^2 * ‚àá¬≤.x·µÄB‚Åª¬πx - 24 * ‚àá¬≤.Œº^4 * ‚àá¬≤.x·µÄB‚Åª·µÄB‚Åª¬πx # d¬≤/dŒº¬≤ [||Ax-b||¬≤]

seminorm(work::NNLSTikhonovRegProblem) = ‚àö(seminorm_sq(work)) # ||x||
seminorm_sq(work::NNLSTikhonovRegProblem) = GC.@preserve work sum(abs2, NNLS.positive_solution(work.nnls_prob.nnls_work)) # ||x||¬≤
‚àáseminorm_sq(work::NNLSTikhonovRegProblem, ‚àá = gradient_temps(work)) = -4 * ‚àá.Œº * ‚àá.x·µÄB‚Åª¬πx # d/dŒº [||x||¬≤]
‚àá¬≤seminorm_sq(work::NNLSTikhonovRegProblem, ‚àá¬≤ = hessian_temps(work)) = -4 * ‚àá¬≤.x·µÄB‚Åª¬πx + 24 * ‚àá¬≤.Œº^2 * ‚àá¬≤.x·µÄB‚Åª·µÄB‚Åª¬πx # d¬≤/dŒº¬≤ [||x||¬≤]

solution_gradnorm(work::NNLSTikhonovRegProblem, ‚àá¬≤ = hessian_temps(work)) = ‚àö(solution_gradnorm_sq(work, ‚àá¬≤)) # ||dx/dŒº|| = ||-2Œº * B‚Åª¬πx|| = 2Œº * ||B‚Åª¬πx||
solution_gradnorm_sq(work::NNLSTikhonovRegProblem, ‚àá¬≤ = hessian_temps(work)) = 4 * ‚àá¬≤.Œº^2 * ‚àá¬≤.x·µÄB‚Åª·µÄB‚Åª¬πx # ||dx/dŒº||¬≤ = ||-2Œº * B‚Åª¬πx||¬≤ = 4Œº¬≤ * x·µÄB‚Åª·µÄB‚Åª¬πx

# L-curve: (Œæ(Œº), Œ∑(Œº)) = (||Ax-b||^2, ||x||^2)
curvature(::typeof(identity), work::NNLSTikhonovRegProblem, ‚àá = gradient_temps(work)) = inv(2 * ‚àá.x·µÄB‚Åª¬πx * ‚àö(1 + ‚àá.Œº^4)^3)

# L-curve: (ŒæÃÑ(Œº), Œ∑ÃÑ(Œº)) = (log||Ax-b||^2, log||x||^2)
function curvature(::typeof(log), work::NNLSTikhonovRegProblem, ‚àá = gradient_temps(work))
    ‚Ñì¬≤ = loss(work) # ‚Ñì¬≤ = ||Ax-b||^2 + Œº¬≤||x||^2 = Œæ¬≤ + Œº¬≤Œ∑¬≤
    Œæ¬≤ = resnorm_sq(work)
    Œ∑¬≤ = seminorm_sq(work)
    Œæ‚Å¥, Œ∑‚Å¥ = Œæ¬≤^2, Œ∑¬≤^2
    CÃÑ = Œæ¬≤ * Œ∑¬≤ * (Œæ¬≤ * Œ∑¬≤ - (2 * ‚àá.x·µÄB‚Åª¬πx) * ‚àá.Œº^2 * ‚Ñì¬≤) / (2 * ‚àá.x·µÄB‚Åª¬πx * ‚àö(Œæ‚Å¥ + ‚àá.Œº^4 * Œ∑‚Å¥)^3)
    return CÃÑ
end

function gradient_temps(work::NNLSTikhonovRegProblem{T}) where {T}
    GC.@preserve work begin
        (; nnls_work) = work.nnls_prob
        B = cholesky!(NNLS.NormalEquation(), nnls_work) # B = A'A + Œº¬≤I = U'U
        x‚Çä = NNLS.positive_solution(nnls_work)
        tmp = uview(work.buffers.y, 1:length(x‚Çä))

        Œº = mu(work)
        copyto!(tmp, x‚Çä)
        NNLS.solve_triangular_system!(tmp, B, Val(true)) # tmp = U'\x
        x·µÄB‚Åª¬πx = sum(abs2, tmp) # x'B\x = x'(U'U)\x = ||U'\x||^2

        return (; Œº, x·µÄB‚Åª¬πx)
    end
end

function hessian_temps(work::NNLSTikhonovRegProblem{T}) where {T}
    GC.@preserve work begin
        (; nnls_work) = work.nnls_prob
        B = cholesky!(NNLS.NormalEquation(), nnls_work) # B = A'A + Œº¬≤I = U'U
        x‚Çä = NNLS.positive_solution(nnls_work)
        tmp = uview(work.buffers.y, 1:length(x‚Çä))

        Œº = mu(work)
        copyto!(tmp, x‚Çä)
        NNLS.solve_triangular_system!(tmp, B, Val(true)) # tmp = U'\x
        x·µÄB‚Åª¬πx = sum(abs2, tmp) # x'B\x = x'(U'U)\x = ||U'\x||^2

        NNLS.solve_triangular_system!(tmp, B, Val(false)) # tmp = U\(U'\x) = (U'U)\x
        x·µÄB‚Åª·µÄB‚Åª¬πx = sum(abs2, tmp) # x'B'\B\x = ||B\x||^2 = ||(U'U)\x||^2

        return (; Œº, x·µÄB‚Åª¬πx, x·µÄB‚Åª·µÄB‚Åª¬πx)
    end
end

function chi2_relerr!(work::NNLSTikhonovRegProblem, œá¬≤target, logŒº, ‚àálogŒº = nothing)
    # NOTE: assumes `solve!(work, Œº)` has been called and that the solution is ready
    Œº = exp(logŒº)
    res¬≤ = resnorm_sq(work)
    relerr = log(res¬≤ / œá¬≤target) # better behaved than res¬≤ / œá¬≤target - 1 for large res¬≤?
    if ‚àálogŒº !== nothing && length(‚àálogŒº) > 0
        ‚àÇres¬≤_‚àÇŒº = ‚àáresnorm_sq(work)
        ‚àÇrelerr_‚àÇlogŒº = Œº * ‚àÇres¬≤_‚àÇŒº / res¬≤
        @inbounds ‚àálogŒº[1] = ‚àÇrelerr_‚àÇlogŒº
    end
    return relerr
end
chi2_relerr‚Åª¬π(œá¬≤target, relerr) = œá¬≤target * exp(relerr)

# Helper struct which wraps `N` caches of `NNLSTikhonovRegProblem` workspaces.
# Useful for optimization problems where the last function call may not be
# the optimium, but perhaps it was one or two calls previous and is still in the
# `NNLSTikhonovRegProblemCache` and a recomputation can be avoided.
struct NNLSTikhonovRegProblemCache{N, W <: NTuple{N}}
    cache::W
    idx::Base.RefValue{Int}
end
function NNLSTikhonovRegProblemCache(A::AbstractMatrix{T}, b::AbstractVector{T}, ::Val{N} = Val(5)) where {T, N}
    cache = ntuple(_ -> NNLSTikhonovRegProblem(A, b), N)
    idx = Ref(1)
    return NNLSTikhonovRegProblemCache(cache, idx)
end
increment_cache_index!(work::NNLSTikhonovRegProblemCache{N}) where {N} = (work.idx[] = mod1(work.idx[] + 1, N))
set_cache_index!(work::NNLSTikhonovRegProblemCache{N}, i) where {N} = (work.idx[] = mod1(i, N))
reset_cache!(work::NNLSTikhonovRegProblemCache{N}) where {N} = foreach(w -> mu!(w, NaN), work.cache)
get_cache(work::NNLSTikhonovRegProblemCache) = work.cache[work.idx[]]

function solve!(work::NNLSTikhonovRegProblemCache, Œº::Real)
    i = findfirst(w -> Œº == mu(w), work.cache)
    if i === nothing
        increment_cache_index!(work)
        solve!(get_cache(work), Œº)
    else
        set_cache_index!(work, i)
    end
    return solution(get_cache(work))
end

####
#### Chi2 method for choosing the Tikhonov regularization parameter
####

struct NNLSChi2RegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W1, W2}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
end
function NNLSChi2RegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    return NNLSChi2RegProblem(A, b, m, n, nnls_prob, nnls_prob_smooth_cache)
end

@inline solution(work::NNLSChi2RegProblem) = solution(get_cache(work.nnls_prob_smooth_cache))
@inline ncomponents(work::NNLSChi2RegProblem) = ncomponents(get_cache(work.nnls_prob_smooth_cache))

@doc raw"""
    lsqnonneg_chi2(A::AbstractMatrix, b::AbstractVector, chi2_target::Real)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||x||_2^2
```

where ``\mu`` is determined by solving:

```math
\chi^2(\mu) = \frac{||AX_{\mu} - b||_2^2}{||AX_{0} - b||_2^2} = \chi^2_{\mathrm{target}}.
```

That is, ``\mu`` is chosen such that the squared residual norm of the regularized problem is `chi2_target`
times larger than the squared residual norm of the unregularized problem.

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data
  - `chi2_target::Real`: Target ``\chi^2(\mu)``; typically a small value, e.g. 1.02 representing a 2% increase

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting ``\chi^2(\mu)``, which should be approximately equal to `chi2_target`
"""
function lsqnonneg_chi2(A::AbstractMatrix, b::AbstractVector, chi2_target::Real, args...; kwargs...)
    work = lsqnonneg_chi2_work(A, b)
    return lsqnonneg_chi2!(work, chi2_target, args...; kwargs...)
end
lsqnonneg_chi2_work(A::AbstractMatrix, b::AbstractVector) = NNLSChi2RegProblem(A, b)

function lsqnonneg_chi2!(work::NNLSChi2RegProblem{T}, chi2_target::T, legacy::Bool = false; method::Symbol = legacy ? :legacy : :bisect) where {T}
    # Non-regularized solution
    solve!(work.nnls_prob)
    x_unreg = solution(work.nnls_prob)
    res¬≤_min = resnorm_sq(work.nnls_prob)

    if res¬≤_min == 0 || ncomponents(work.nnls_prob) == 0
        # 1. If non-regularized solution is exact, the only solution to res¬≤(Œº) = chi2_target * res¬≤_min = 0 is Œº = 0, since res¬≤(Œº) > 0 for all Œº > 0.
        # 2. If non-regularized solution is zero, any value of Œº > 0 also results in x(Œº) = 0, and so res¬≤(Œº) = chi2_target * res¬≤_min has either no solutions if chi2_target > 1, or infinitely many solutions if chi2_target = 1; choose Œº = 0 and chi2_target = 1.
        x_final = x_unreg
        return (; x = x_final, mu = zero(T), chi2 = one(T))
    end

    # Prepare to solve
    res¬≤_target = chi2_target * res¬≤_min
    reset_cache!(work.nnls_prob_smooth_cache)

    if method === :legacy
        # Use the legacy algorithm: double Œº starting from an initial guess, then interpolate the root using a cubic spline fit
        mu_final, res¬≤_final = chi2_search_from_minimum(res¬≤_min, chi2_target; legacy) do Œº
            Œº == 0 && return res¬≤_min
            solve!(work.nnls_prob_smooth_cache, Œº)
            return resnorm_sq(get_cache(work.nnls_prob_smooth_cache))
        end
        if mu_final == 0
            x_final = x_unreg
        else
            x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
        end

    elseif method === :bisect
        f = function (logŒº)
            solve!(work.nnls_prob_smooth_cache, exp(logŒº))
            return chi2_relerr!(get_cache(work.nnls_prob_smooth_cache), res¬≤_target, logŒº)
        end

        # Find bracketing interval containing root, then perform bisection search with slightly higher tolerance to not waste f evals
        a, b, fa, fb = bracket_root_monotonic(f, T(-4.0), T(1.0); dilate = T(1.5), mono = +1, maxiters = 6) # maxiters = 100

        if fa * fb < 0
            # Bracketing interval found
            a, fa, c, fc, b, fb = bisect_root(f, a, b, fa, fb; xatol = T(0.05), xrtol = T(0.0), ftol = (chi2_target - 1) / 100) # maxiters = 100

            # Root of secant line through `(a, fa), (b, fb)` or `(c, fc), (b, fb)` to improve bisection accuracy
            tmp = fa * fc < 0 ? root_real_linear(a, c, fa, fc) : fc * fb < 0 ? root_real_linear(c, b, fc, fb) : T(NaN)
            d, fd = isnan(tmp) ? (c, fc) : (tmp, f(tmp))

            # Return regularization parameter with lowest abs(relerr)
            logmu_final, relerr_final = abs(fd) < abs(fc) ? (d, fd) : (c, fc)
        else
            # No bracketing interval found; choose point with smallest value of f (note: this branch should never be reached)
            logmu_final, relerr_final = !isfinite(fa) ? (b, fb) : !isfinite(fb) ? (a, fa) : abs(fa) < abs(fb) ? (a, fa) : (b, fb)
        end

        if isfinite(relerr_final)
            mu_final, res¬≤_final = exp(logmu_final), chi2_relerr‚Åª¬π(res¬≤_target, relerr_final)
            x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
        else
            x_final, mu_final, res¬≤_final = x_unreg, zero(T), one(T)
        end

    elseif method === :brent
        f = function (logŒº)
            solve!(work.nnls_prob_smooth_cache, exp(logŒº))
            return chi2_relerr!(get_cache(work.nnls_prob_smooth_cache), res¬≤_target, logŒº)
        end

        # Find bracketing interval containing root
        a, b, fa, fb = bracket_root_monotonic(f, T(-4.0), T(1.0); dilate = T(1.5), mono = +1, maxiters = 100)

        if fa * fb < 0
            # Find root using Brent's method
            logmu_final, relerr_final = brent_root(f, a, b, fa, fb; xatol = T(0.0), xrtol = T(0.0), ftol = (chi2_target - 1) / 1000, maxiters = 100)
        else
            # No bracketing interval found; choose point with smallest value of f (note: this branch should never be reached)
            logmu_final, relerr_final = !isfinite(fa) ? (b, fb) : !isfinite(fb) ? (a, fa) : abs(fa) < abs(fb) ? (a, fa) : (b, fb)
        end

        if isfinite(relerr_final)
            mu_final, res¬≤_final = exp(logmu_final), chi2_relerr‚Åª¬π(res¬≤_target, relerr_final)
            x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
        else
            x_final, mu_final, res¬≤_final = x_unreg, zero(T), one(T)
        end
    else
        error("Unknown root-finding method: :$method")
    end

    return (; x = x_final, mu = mu_final, chi2 = res¬≤_final / res¬≤_min)
end

function chi2_search_from_minimum(f, res¬≤min::T, œá¬≤fact::T, Œºmin::T = T(1e-3), Œºfact = T(2.0); legacy = false) where {T}
    # Minimize energy of spectrum; loop to find largest Œº that keeps chi-squared in desired range
    Œº_cache = T[zero(T)]
    res¬≤_cache = T[res¬≤min]
    Œºnew = Œºmin
    while true
        # Cache function value at Œº = Œºnew
        res¬≤new = f(Œºnew)
        push!(Œº_cache, Œºnew)
        push!(res¬≤_cache, res¬≤new)

        # Break when œá¬≤fact reached, else increase regularization
        (res¬≤new >= œá¬≤fact * res¬≤min) && break
        Œºnew *= Œºfact
    end

    # Solve res¬≤(Œº) = œá¬≤fact * res¬≤min using a spline fitting root finding method
    if legacy
        # Legacy algorithm fits spline to all (Œº, res¬≤) values observed, including for Œº=0.
        # This poses several problems:
        #   1) while unlikely, it is possible for the spline to return a negative regularization parameter
        #   2) the Œº values are exponentially spaced, leading to poorly conditioned splines
        Œº = spline_root_legacy(Œº_cache, res¬≤_cache, œá¬≤fact * res¬≤min)
    else
        if length(Œº_cache) == 2
            # Solution is contained in [0,Œºmin]; `spline_root` with two points performs root finding via simple linear interpolation
            Œº = spline_root(Œº_cache, res¬≤_cache, œá¬≤fact * res¬≤min; deg_spline = 1)
            Œº = isnan(Œº) ? Œºmin : Œº
        else
            # Perform spline fit on log-log scale on data with Œº > 0. This solves the above problems with the legacy algorithm:
            #   1) Root is found in terms of logŒº, guaranteeing Œº > 0
            #   2) logŒº is linearly spaced, leading to well-conditioned splines
            logŒº = spline_root(log.(Œº_cache[2:end]), log.(res¬≤_cache[2:end]), log(œá¬≤fact * res¬≤min); deg_spline = 1)
            Œº = isnan(logŒº) ? Œºmin : exp(logŒº)
        end
    end

    # Compute the final regularized solution
    res¬≤ = f(Œº)

    return Œº, res¬≤
end

####
#### Morozov discrepency principle (MDP) method for choosing the Tikhonov regularization parameter
####

struct NNLSMDPRegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W1, W2}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
end
function NNLSMDPRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    return NNLSMDPRegProblem(A, b, m, n, nnls_prob, nnls_prob_smooth_cache)
end

@inline solution(work::NNLSMDPRegProblem) = solution(get_cache(work.nnls_prob_smooth_cache))
@inline ncomponents(work::NNLSMDPRegProblem) = ncomponents(get_cache(work.nnls_prob_smooth_cache))

@doc raw"""
    lsqnonneg_mdp(A::AbstractMatrix, b::AbstractVector, Œ¥::Real)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||x||_2^2
```

where ``\mu`` is chosen using Morozov's Discrepency Principle (MDP)[1,2]:

```math
\mu = \operatorname{sup}\; \left\{ \nu \ge 0 : ||AX_{\nu} - b|| \le \delta \right\}.
```

That is, ``\mu`` is maximized subject to the constraint that the residual norm of the regularized problem is at most ``\delta``[1].

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data
  - `Œ¥::Real`: Upper bound on regularized residual norm

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting increase in residual norm relative to the unregularized ``\mu = 0`` solution

# References

  1. Morozov VA. Methods for Solving Incorrectly Posed Problems. Springer Science & Business Media, 2012.
  2. Clason C, Kaltenbacher B, Resmerita E. Regularization of Ill-Posed Problems with Non-negative Solutions. In: Bauschke HH, Burachik RS, Luke DR (eds) Splitting Algorithms, Modern Operator Theory, and Applications. Cham: Springer International Publishing, pp. 113‚Äì135.
"""
function lsqnonneg_mdp(A::AbstractMatrix, b::AbstractVector, Œ¥::Real, args...; kwargs...)
    work = lsqnonneg_mdp_work(A, b)
    return lsqnonneg_mdp!(work, Œ¥, args...; kwargs...)
end
lsqnonneg_mdp_work(A::AbstractMatrix, b::AbstractVector) = NNLSMDPRegProblem(A, b)

function lsqnonneg_mdp!(work::NNLSMDPRegProblem{T}, Œ¥::T) where {T}
    # Non-regularized solution
    solve!(work.nnls_prob)
    x_unreg = solution(work.nnls_prob)
    res¬≤_min = resnorm_sq(work.nnls_prob)

    #TODO: throw error if Œ¥ > ||b||, since ||A * x(Œº) - b|| <= ||b|| when A, x, b are componentwise nonnegative, or return... what?
    # if Œ¥ >= norm(work.nnls_prob.b)
    #     error("Œ¥ = $Œ¥ is greater than the norm of the data vector ||b|| = $(norm(work.nnls_prob.b))")
    # end
    if Œ¥ <= res¬≤_min
        x_final = x_unreg
        return (; x = x_final, mu = zero(T), chi2 = one(T))
    end

    # Prepare to solve
    reset_cache!(work.nnls_prob_smooth_cache)

    function f(logŒº)
        solve!(work.nnls_prob_smooth_cache, exp(logŒº))
        return resnorm_sq(get_cache(work.nnls_prob_smooth_cache)) - Œ¥^2
    end

    # Find bracketing interval containing root
    a, b, fa, fb = bracket_root_monotonic(f, T(-4.0), T(1.0); dilate = T(1.5), mono = +1, maxiters = 100)

    if fa * fb < 0
        # Find root using Brent's method
        logmu_final, err_final = brent_root(f, a, b, fa, fb; xatol = T(0.0), xrtol = T(0.0), ftol = Œ¥ / 1000, maxiters = 100)
    else
        # No bracketing interval found; choose point with smallest value of f (note: this branch should never be reached)
        logmu_final, err_final = !isfinite(fa) ? (b, fb) : !isfinite(fb) ? (a, fa) : abs(fa) < abs(fb) ? (a, fa) : (b, fb)
    end

    if isfinite(err_final)
        mu_final, res¬≤_final = exp(logmu_final), Œ¥^2 + err_final
        x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
    else
        x_final, mu_final, res¬≤_final = x_unreg, zero(T), one(T)
    end

    return (; x = x_final, mu = mu_final, chi2 = res¬≤_final / res¬≤_min)
end

####
#### L-curve method for choosing the Tikhonov regularization parameter
####

struct NNLSLCurveRegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W1, W2, C1, C2}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
    lsqnonneg_lcurve_fun_cache::C1
    lcurve_corner_caches::C2
end
function NNLSLCurveRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    lsqnonneg_lcurve_fun_cache = GrowableCache{T, SVector{2, T}}(64, isapprox)
    lcurve_corner_caches = (
        GrowableCache{T, LCurveCornerPoint{T}}(64, isapprox),
        GrowableCache{T, LCurveCornerState{T}}(64, isapprox),
    )
    return NNLSLCurveRegProblem(A, b, m, n, nnls_prob, nnls_prob_smooth_cache, lsqnonneg_lcurve_fun_cache, lcurve_corner_caches)
end

@inline solution(work::NNLSLCurveRegProblem) = solution(get_cache(work.nnls_prob_smooth_cache))
@inline ncomponents(work::NNLSLCurveRegProblem) = ncomponents(get_cache(work.nnls_prob_smooth_cache))

@doc raw"""
    lsqnonneg_lcurve(A::AbstractMatrix, b::AbstractVector)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||L x||_2^2
```

where ``L`` is the identity matrix, and ``\mu`` is chosen by locating the corner of the "L-curve"[1].
Details of L-curve theory can be found in Hansen (1992)[2].

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting increase in residual norm relative to the unregularized ``\mu = 0`` solution

# References

  1. A. Cultrera and L. Callegaro, "A simple algorithm to find the L-curve corner in the regularization of ill-posed inverse problems". IOPSciNotes, vol. 1, no. 2, p. 025004, Aug. 2020, https://doi.org/10.1088/2633-1357/abad0d.
  2. Hansen, P.C., 1992. Analysis of Discrete Ill-Posed Problems by Means of the L-Curve. SIAM Review, 34(4), 561-580, https://doi.org/10.1137/1034115.
"""
function lsqnonneg_lcurve(A::AbstractMatrix, b::AbstractVector)
    work = lsqnonneg_lcurve_work(A, b)
    return lsqnonneg_lcurve!(work)
end
lsqnonneg_lcurve_work(A::AbstractMatrix, b::AbstractVector) = NNLSLCurveRegProblem(A, b)

function lsqnonneg_lcurve!(work::NNLSLCurveRegProblem{T, N}) where {T, N}
    # Compute the regularization using the L-curve method
    reset_cache!(work.nnls_prob_smooth_cache)

    # A point on the L-curve is given by (Œæ(Œº), Œ∑(Œº)) = (log||Ax-b||^2, log||x||^2)
    #   Note: Squaring the norms is convenient for computing gradients of (Œæ(Œº), Œ∑(Œº));
    #         this scales the L-curve, but does not change Œº* = argmax C(Œæ(Œº), Œ∑(Œº)).
    function f_lcurve(logŒº)
        solve!(work.nnls_prob_smooth_cache, exp(logŒº))
        Œæ = log(resnorm_sq(get_cache(work.nnls_prob_smooth_cache)))
        Œ∑ = log(seminorm_sq(get_cache(work.nnls_prob_smooth_cache)))
        return SA{T}[Œæ, Œ∑]
    end

    # Build cached function and solve
    f_lcurve_cached = CachedFunction(f_lcurve, empty!(work.lsqnonneg_lcurve_fun_cache))
    f = LCurveCornerCachedFunction(f_lcurve_cached, empty!.(work.lcurve_corner_caches)...)

    logmu_bounds = (T(-8), T(2))
    logmu_final = lcurve_corner(f, logmu_bounds...)

    # Return the final regularized solution
    mu_final = exp(logmu_final)
    x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
    x_unreg = solve!(work.nnls_prob)
    chi2_final = resnorm_sq(get_cache(work.nnls_prob_smooth_cache)) / resnorm_sq(work.nnls_prob)

    return (; x = x_final, mu = mu_final, chi2 = chi2_final)
end

struct LCurveCornerState{T}
    x‚Éó::SVector{4, T} # grid of regularization parameters
    P‚Éó::SVector{4, SVector{2, T}} # points (residual norm, solution seminorm) evaluated at x‚Éó
end
@inline Base.iterate(s::LCurveCornerState, args...) = iterate((s.x‚Éó, s.P‚Éó), args...)

struct LCurveCornerPoint{T}
    P::SVector{2, T} # grid point
    C::T # curvature
end
LCurveCornerPoint(P::SVector{2, T}) where {T} = LCurveCornerPoint(P, T(-Inf))
@inline Base.iterate(p::LCurveCornerPoint, args...) = iterate((p.P, p.C), args...)

struct LCurveCornerCachedFunction{T, F <: CachedFunction{T, SVector{2, T}}, C1 <: GrowableCache{T, LCurveCornerPoint{T}}, C2 <: GrowableCache{T, LCurveCornerState{T}}}
    f::F
    point_cache::C1
    state_cache::C2
end
@inline Base.empty!(f::LCurveCornerCachedFunction) = (empty!(f.f); empty!(f.point_cache); empty!(f.state_cache); f)
@inline (f::LCurveCornerCachedFunction{T})(x::T) where {T} = f.f(x)

@doc raw"""
    lcurve_corner(f, xlow, xhigh)

Find the corner of the L-curve via curvature maximization using a modified version of Algorithm 1 from Cultrera and Callegaro (2020)[1].

# References

  1. A. Cultrera and L. Callegaro, "A simple algorithm to find the L-curve corner in the regularization of ill-posed inverse problems". IOPSciNotes, vol. 1, no. 2, p. 025004, Aug. 2020, https://doi.org/10.1088/2633-1357/abad0d.
"""
function lcurve_corner(f::LCurveCornerCachedFunction{T}, xlow::T = -8.0, xhigh::T = 2.0; xtol = 0.05, Ptol = 0.05, Ctol = 0.01, backtracking = true) where {T}
    # Initialize state
    state = initial_state(f, T(xlow), T(xhigh))

    # Tolerances are relative to initial curve size
    Ptopleft, Pbottomright = state.P‚Éó[1], state.P‚Éó[4]
    Pdiam = norm(Ptopleft - Pbottomright)
    Ptol = Pdiam * T(Ptol) # convergence occurs when diameter of L-curve state is less than Ptol
    Ctol = Pdiam * T(Ctol) # note: *not* a tolerance on curvature, but on the minimum diameter of the L-curve state used to estimate curvature (see `Pfilter` below)

    # For very small regularization points on the L-curve may be extremely close, leading to
    # numerically unstable curvature estimates. Assign these points -Inf curvature.
    Pfilter = P -> min(norm(P - Ptopleft), norm(P - Pbottomright)) > T(Ctol)
    update_curvature!(f, state, Pfilter)

    # msg(s, state) = (@info "$s: [x‚Éó, P‚Éó, C‚Éó] = "; display(hcat(state.x‚Éó, state.P‚Éó, [f.point_cache[x].C for x in state.x‚Éó])))
    # msg("Starting", state)

    iter = 0
    while true
        iter += 1
        if backtracking
            # Find state with minimum diameter which contains the current best estimate maximum curvature point
            (x, (P, C)), _, _ = mapfindmax(T, ((x, (P, C)),) -> C, pairs(f.point_cache))
            for (_, s) in f.state_cache
                if (s.x‚Éó[2] == x || s.x‚Éó[3] == x) && abs(s.x‚Éó[4] - s.x‚Éó[1]) <= abs(state.x‚Éó[4] - state.x‚Éó[1])
                    state = s
                end
            end
        end

        # Move state toward region of lower curvature
        if f.point_cache[state.x‚Éó[2]].C > f.point_cache[state.x‚Éó[3]].C
            state = move_left(f, state)
            update_curvature!(f, state, Pfilter)
            # msg("C‚ÇÇ > C‚ÇÉ; moved left", state)
        else
            state = move_right(f, state)
            update_curvature!(f, state, Pfilter)
            # msg("C‚ÇÉ ‚â• C‚ÇÇ; moved right", state)
        end
        backtracking && push!(f.state_cache, (iter, state))
        is_converged(state; xtol = T(xtol), Ptol = T(Ptol)) && break
    end

    x = f.point_cache[state.x‚Éó[2]].C > f.point_cache[state.x‚Éó[3]].C ? state.x‚Éó[2] : state.x‚Éó[3]
    # msg("Converged", state)

    return x
end

function initial_state(f::LCurveCornerCachedFunction{T}, x‚ÇÅ::T, x‚ÇÑ::T) where {T}
    x‚ÇÇ = (T(œÜ) * x‚ÇÅ + x‚ÇÑ) / (T(œÜ) + 1)
    x‚ÇÉ = x‚ÇÅ + (x‚ÇÑ - x‚ÇÇ)
    x‚Éó = SA[x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ]
    P‚Éó = SA[f(x‚ÇÅ), f(x‚ÇÇ), f(x‚ÇÉ), f(x‚ÇÑ)]
    Base.Cartesian.@nexprs 4 i -> push!(f.point_cache, (x‚Éó[i], LCurveCornerPoint(P‚Éó[i])))
    return LCurveCornerState(x‚Éó, P‚Éó)
end

is_converged(state::LCurveCornerState; xtol, Ptol) = abs(state.x‚Éó[4] - state.x‚Éó[1]) < xtol || norm(state.P‚Éó[1] - state.P‚Éó[4]) < Ptol

function move_left(f::LCurveCornerCachedFunction{T}, state::LCurveCornerState{T}) where {T}
    (; x‚Éó, P‚Éó) = state
    x‚Éó = SA[x‚Éó[1], (T(œÜ)*x‚Éó[1]+x‚Éó[3])/(T(œÜ)+1), x‚Éó[2], x‚Éó[3]]
    P‚Éó = SA[P‚Éó[1], f(x‚Éó[2]), P‚Éó[2], P‚Éó[3]] # only P‚Éó[2] is recalculated
    return LCurveCornerState{T}(x‚Éó, P‚Éó)
end

function move_right(f::LCurveCornerCachedFunction{T}, state::LCurveCornerState{T}) where {T}
    (; x‚Éó, P‚Éó) = state
    x‚Éó = SA[x‚Éó[2], x‚Éó[3], x‚Éó[2]+(x‚Éó[4]-x‚Éó[3]), x‚Éó[4]]
    P‚Éó = SA[P‚Éó[2], P‚Éó[3], f(x‚Éó[3]), P‚Éó[4]] # only P‚Éó[3] is recalculated
    return LCurveCornerState(x‚Éó, P‚Éó)
end

function update_curvature!(f::LCurveCornerCachedFunction{T}, state::LCurveCornerState{T}, Pfilter = nothing) where {T}
    (; x‚Éó, P‚Éó) = state
    for i in 1:4
        x, P, C = x‚Éó[i], P‚Éó[i], T(-Inf)
        if Pfilter === nothing || Pfilter(P)
            # Compute curvature from nearest neighbours
            x‚Çã, x‚Çä = T(-Inf), T(+Inf)
            P‚Çã, P‚Çä = P, P
            for (_x, (_P, _)) in pairs(f.point_cache)
                (x‚Çã < _x < x) && ((x‚Çã, P‚Çã) = (_x, _P))
                (x < _x < x‚Çä) && ((x‚Çä, P‚Çä) = (_x, _P))
            end
            C = menger(P‚Çã, P, P‚Çä)
        end
        f.point_cache[x] = LCurveCornerPoint(P, C)
    end
    return state
end

function menger(P‚±º::V, P‚Çñ::V, P‚Çó::V) where {V <: SVector{2}}
    Œî‚±º‚Çñ, Œî‚Çñ‚Çó, Œî‚Çó‚±º = P‚±º - P‚Çñ, P‚Çñ - P‚Çó, P‚Çó - P‚±º
    PÃÑ‚±ºPÃÑ‚Çñ, PÃÑ‚ÇñPÃÑ‚Çó, PÃÑ‚ÇóPÃÑ‚±º = Œî‚±º‚Çñ ‚ãÖ Œî‚±º‚Çñ, Œî‚Çñ‚Çó ‚ãÖ Œî‚Çñ‚Çó, Œî‚Çó‚±º ‚ãÖ Œî‚Çó‚±º
    C‚Çñ = 2 * (Œî‚±º‚Çñ √ó Œî‚Çñ‚Çó) / ‚àö(PÃÑ‚±ºPÃÑ‚Çñ * PÃÑ‚ÇñPÃÑ‚Çó * PÃÑ‚ÇóPÃÑ‚±º)
    return C‚Çñ
end

function menger(f; h = 1e-3)
    function menger_curvature_inner(x)
        f‚±º, f‚Çñ, f‚Çó = f(x - h), f(x), f(x + h)
        P‚±º, P‚Çñ, P‚Çó = SA[x-h, f‚±º], SA[x, f‚Çñ], SA[x+h, f‚Çó]
        return menger(P‚±º, P‚Çñ, P‚Çó)
    end
end

function menger(x, y; h = 1e-3)
    function menger_curvature_inner(t)
        x‚Çã, x‚ÇÄ, x‚Çä = x(t - h), x(t), x(t + h)
        y‚Çã, y‚ÇÄ, y‚Çä = y(t - h), y(t), y(t + h)
        x‚Ä≤, x‚Ä≤‚Ä≤ = (x‚Çä - x‚Çã) / 2h, (x‚Çä - 2x‚ÇÄ + x‚Çã) / h^2
        y‚Ä≤, y‚Ä≤‚Ä≤ = (y‚Çä - y‚Çã) / 2h, (y‚Çä - 2y‚ÇÄ + y‚Çã) / h^2
        return (x‚Ä≤ * y‚Ä≤‚Ä≤ - y‚Ä≤ * x‚Ä≤‚Ä≤) / ‚àö((x‚Ä≤^2 + y‚Ä≤^2)^3)
    end
end

#=
lin_interp(x, x‚ÇÅ, x‚ÇÇ, y‚ÇÅ, y‚ÇÇ) = y‚ÇÅ + (y‚ÇÇ - y‚ÇÅ) * (x - x‚ÇÅ) / (x‚ÇÇ - x‚ÇÅ)
exp_interp(x, x‚ÇÅ, x‚ÇÇ, y‚ÇÅ, y‚ÇÇ) = y‚ÇÅ + log1p(expm1(y‚ÇÇ - y‚ÇÅ) * (x - x‚ÇÅ) / (x‚ÇÇ - x‚ÇÅ))

function menger(x::Dierckx.Spline1D, y::Dierckx.Spline1D)
    function menger_curvature_inner(t)
        x‚Ä≤  = Dierckx.derivative(x, t; nu = 1)
        x‚Ä≤‚Ä≤ = Dierckx.derivative(x, t; nu = 2)
        y‚Ä≤  = Dierckx.derivative(y, t; nu = 1)
        y‚Ä≤‚Ä≤ = Dierckx.derivative(y, t; nu = 2)
        return (x‚Ä≤ * y‚Ä≤‚Ä≤ - y‚Ä≤ * x‚Ä≤‚Ä≤) / ‚àö((x‚Ä≤^2 + y‚Ä≤^2)^3)
    end
end

function menger(y::Dierckx.Spline1D)
    function menger_curvature_inner(t)
        y‚Ä≤  = Dierckx.derivative(y, t; nu = 1)
        y‚Ä≤‚Ä≤ = Dierckx.derivative(y, t; nu = 2)
        return y‚Ä≤‚Ä≤ / ‚àö((1 + y‚Ä≤^2)^3)
    end
end

function menger(x‚±º::T, x‚Çñ::T, x‚Çó::T, P‚±º::V, P‚Çñ::V, P‚Çó::V; interp_uniform = true, linear_deriv = true) where {T, V <: SVector{2, T}}
    if interp_uniform
        h = min(abs(x‚Çñ - x‚±º), abs(x‚Çó - x‚Çñ)) / T(œÜ)
        h‚Çã = h‚Çä = h
        x‚Çã, x‚ÇÄ, x‚Çä = x‚Çñ - h, x‚Çñ, x‚Çñ + h
        P‚ÇÄ = P‚Çñ
        P‚Çã = exp_interp.(x‚Çã, x‚±º, x‚Çñ, P‚±º, P‚Çñ)
        P‚Çä = exp_interp.(x‚Çä, x‚Çñ, x‚Çó, P‚Çñ, P‚Çó)
    else
        P‚Çã, P‚ÇÄ, P‚Çä = P‚±º, P‚Çñ, P‚Çó
        x‚Çã, x‚ÇÄ, x‚Çä = x‚±º, x‚Çñ, x‚Çó
        h‚Çã, h‚Çä = x‚ÇÄ - x‚Çã, x‚Çä - x‚ÇÄ
    end
    Œæ‚Çã, Œæ‚ÇÄ, Œæ‚Çä = P‚Çã[1], P‚ÇÄ[1], P‚Çä[1]
    Œ∑‚Çã, Œ∑‚ÇÄ, Œ∑‚Çä = P‚Çã[2], P‚ÇÄ[2], P‚Çä[2]

    if linear_deriv
        Œæ‚Ä≤ = (Œæ‚Çä - Œæ‚Çã) / (h‚Çä + h‚Çã)
        Œ∑‚Ä≤ = (Œ∑‚Çä - Œ∑‚Çã) / (h‚Çä + h‚Çã)
    else
        Œæ‚Ä≤ = (h‚Çã^2 * Œæ‚Çä + (h‚Çä + h‚Çã) * (h‚Çä - h‚Çã) * Œæ‚ÇÄ - h‚Çä^2 * Œæ‚Çã) / (h‚Çä * h‚Çã * (h‚Çä + h‚Çã))
        Œ∑‚Ä≤ = (h‚Çã^2 * Œ∑‚Çä + (h‚Çä + h‚Çã) * (h‚Çä - h‚Çã) * Œ∑‚ÇÄ - h‚Çä^2 * Œ∑‚Çã) / (h‚Çä * h‚Çã * (h‚Çä + h‚Çã))
    end

    Œæ‚Ä≤‚Ä≤ = 2 * (h‚Çã * Œæ‚Çä - (h‚Çä + h‚Çã) * Œæ‚ÇÄ + h‚Çä * Œæ‚Çã) / (h‚Çä * h‚Çã * (h‚Çä + h‚Çã))
    Œ∑‚Ä≤‚Ä≤ = 2 * (h‚Çã * Œ∑‚Çä - (h‚Çä + h‚Çã) * Œ∑‚ÇÄ + h‚Çä * Œ∑‚Çã) / (h‚Çä * h‚Çã * (h‚Çä + h‚Çã))

    return (Œæ‚Ä≤ * Œ∑‚Ä≤‚Ä≤ - Œ∑‚Ä≤ * Œæ‚Ä≤‚Ä≤) / ‚àö((Œæ‚Ä≤^2 + Œ∑‚Ä≤^2)^3)
end

function maximize_curvature(state::LCurveCornerState{T}) where {T}
    # Maximize curvature and transform back from t-space to x-space
    (; x‚Éó, P‚Éó) = state
    t‚ÇÅ, t‚ÇÇ, t‚ÇÉ, t‚ÇÑ = T(0), 1 / T(3), 2 / T(3), T(1)
    t_opt, P_opt, C_opt = maximize_curvature(P‚Éó...)
    x_opt =
        t‚ÇÅ <= t_opt < t‚ÇÇ ? lin_interp(t_opt, t‚ÇÅ, t‚ÇÇ, x‚Éó[1], x‚Éó[2]) :
        t‚ÇÇ <= t_opt < t‚ÇÉ ? lin_interp(t_opt, t‚ÇÇ, t‚ÇÉ, x‚Éó[2], x‚Éó[3]) :
        lin_interp(t_opt, t‚ÇÉ, t‚ÇÑ, x‚Éó[3], x‚Éó[4])
    return x_opt
end

function maximize_curvature(P‚ÇÅ::V, P‚ÇÇ::V, P‚ÇÉ::V, P‚ÇÑ::V; bezier = true) where {T, V <: SVector{2, T}}
    # Analytically maximize curvature of parametric cubic spline fit to data.
    #   see: https://cs.stackexchange.com/a/131032
    a, e = P‚ÇÅ[1], P‚ÇÅ[2]
    b, f = P‚ÇÇ[1], P‚ÇÇ[2]
    c, g = P‚ÇÉ[1], P‚ÇÉ[2]
    d, h = P‚ÇÑ[1], P‚ÇÑ[2]

    if bezier
        Œæ = t -> a * (1 - t)^3 + 3 * b * (1 - t)^2 * t + 3 * c * (1 - t) * t^2 + d * t^3
        Œ∑ = t -> e * (1 - t)^3 + 3 * f * (1 - t)^2 * t + 3 * g * (1 - t) * t^2 + h * t^3
        P = t -> SA{T}[Œæ(t), Œ∑(t)]

        # In order to keep the equations in a more compact form, introduce the following substitutions:
        m = d - 3 * c + 3 * b - a
        n = c - 2 * b + a
        o = b - a
        p = h - 3 * g + 3 * f - e
        q = g - 2 * f + e
        r = f - e
    else
        Œæ = t -> a * (t - 1 / 3) * (t - 2 / 3) * (t - 1 / 1) / T((0 / 1 - 1 / 3) * (0 / 1 - 2 / 3) * (0 / 1 - 1 / 1)) + # (-9  * a * t^3)/2  + (+18 * a * t^2)/2 + (-11 * a * t)/2 + a
                 b * (t - 0 / 1) * (t - 2 / 3) * (t - 1 / 1) / T((1 / 3 - 0 / 1) * (1 / 3 - 2 / 3) * (1 / 3 - 1 / 1)) + # (+27 * b * t^3)/2  + (-45 * b * t^2)/2 + (+18 * b * t)/2 +
                 c * (t - 0 / 1) * (t - 1 / 3) * (t - 1 / 1) / T((2 / 3 - 0 / 1) * (2 / 3 - 1 / 3) * (2 / 3 - 1 / 1)) + # (-27 * c * t^3)/2  + (+36 * c * t^2)/2 + ( -9 * c * t)/2 +
                 d * (t - 0 / 1) * (t - 1 / 3) * (t - 2 / 3) / T((1 / 1 - 0 / 1) * (1 / 1 - 1 / 3) * (1 / 1 - 2 / 3))   # (+9  * d * t^3)/2  + ( -9 * d * t^2)/2 + ( +2 * d * t)/2
        Œ∑ = t -> e * (t - 1 / 3) * (t - 2 / 3) * (t - 1 / 1) / T((0 / 1 - 1 / 3) * (0 / 1 - 2 / 3) * (0 / 1 - 1 / 1)) + # (-9  * e * t^3)/2  + (+18 * e * t^2)/2 + (-11 * e * t)/2 + e
                 f * (t - 0 / 1) * (t - 2 / 3) * (t - 1 / 1) / T((1 / 3 - 0 / 1) * (1 / 3 - 2 / 3) * (1 / 3 - 1 / 1)) + # (+27 * f * t^3)/2  + (-45 * f * t^2)/2 + (+18 * f * t)/2 +
                 g * (t - 0 / 1) * (t - 1 / 3) * (t - 1 / 1) / T((2 / 3 - 0 / 1) * (2 / 3 - 1 / 3) * (2 / 3 - 1 / 1)) + # (-27 * g * t^3)/2  + (+36 * g * t^2)/2 + ( -9 * g * t)/2 +
                 h * (t - 0 / 1) * (t - 1 / 3) * (t - 2 / 3) / T((1 / 1 - 0 / 1) * (1 / 1 - 1 / 3) * (1 / 1 - 2 / 3))   # (+9  * h * t^3)/2  + ( -9 * h * t^2)/2 + ( +2 * h * t)/2
        P = t -> SA{T}[Œæ(t), Œ∑(t)]

        # In order to keep the equations in a more compact form, introduce the following substitutions:
        m = (-9 * a + 27 * b - 27 * c + 9 * d) / 2
        n = (+18 * a - 45 * b + 36 * c - 9 * d) / 6
        o = (-11 * a + 18 * b - 9 * c + 2 * d) / 6
        p = (-9 * e + 27 * f - 27 * g + 9 * h) / 2
        q = (+18 * e - 45 * f + 36 * g - 9 * h) / 6
        r = (-11 * e + 18 * f - 9 * g + 2 * h) / 6
    end

    # This leads to the following simplified derivatives:
    Œæ‚Ä≤  = t -> 3 * (m * t^2 + 2 * n * t + o)
    Œæ‚Ä≤‚Ä≤ = t -> 6 * (m * t + n)
    Œ∑‚Ä≤  = t -> 3 * (p * t^2 + 2 * q * t + r)
    Œ∑‚Ä≤‚Ä≤ = t -> 6 * (p * t + q)

    # Curvature and its derivative:
    k  = t -> ((3 * (m * t^2 + 2 * n * t + o)) * (6 * (p * t + q)) - (3 * (p * t^2 + 2 * q * t + r)) * (6 * (m * t + n))) / ((3 * (m * t^2 + 2 * n * t + o))^2 + (3 * (p * t^2 + 2 * q * t + r))^2)^(3 / 2)
    k‚Ä≤ = t -> (-18 * m * (p * t^2 + 2 * q * t + r) + 18 * p * (m * t^2 + 2 * n * t + o) - 18 * (m * t + n) * (2 * p * t + 2 * q) + 18 * (2 * m * t + 2 * n) * (p * t + q)) / (9 * (p * t^2 + 2 * q * t + r)^2 + 9 * (m * t^2 + 2 * n * t + o)^2)^(3 / 2) - (3 * (18 * (p * t + q) * (m * t^2 + 2 * n * t + o) - 18 * (m * t + n) * (p * t^2 + 2 * q * t + r)) * (18 * (2 * p * t + 2 * q) * (p * t^2 + 2 * q * t + r) + 18 * (2 * m * t + 2 * n) * (m * t^2 + 2 * n * t + o))) / (2 * (9 * (p * t^2 + 2 * q * t + r)^2 + 9 * (m * t^2 + 2 * n * t + o)^2)^(5 / 2))

    # Solve analytically
    coeffs = MVector{6, Complex{T}}(
        (1296 * m * p^2 + 1296 * m^3) * q - 1296 * n * p^3 - 1296 * m^2 * n * p,
        (1620 * m * p^2 + 1620 * m^3) * r + 3240 * m * p * q^2 + (3240 * m^2 * n - 3240 * n * p^2) * q - 1620 * o * p^3 + ((-1620 * m^2 * o) - 3240 * m * n^2) * p,
        (5184 * m * p * q + 1296 * n * p^2 + 6480 * m^2 * n) * r + 1296 * m * q^3 - 1296 * n * p * q^2 + ((-6480 * o * p^2) - 1296 * m^2 * o + 1296 * m * n^2) * q + ((-5184 * m * n * o) - 1296 * n^3) * p,
        1296 * m * p * r^2 + (1944 * m * q^2 + 6480 * n * p * q - 1296 * o * p^2 + 1296 * m^2 * o + 8424 * m * n^2) * r - 8424 * o * p * q^2 - 6480 * m * n * o * q + ((-1296 * m * o^2) - 1944 * n^2 * o) * p,
        2592 * n * p * r^2 + (3888 * n * q^2 - 2592 * o * p * q + 2592 * m * n * o + 3888 * n^3) * r - 3888 * o * q^3 + ((-2592 * m * o^2) - 3888 * n^2 * o) * q,
        -324 * m * r^3 + (1944 * n * q + 324 * o * p) * r^2 + ((-1944 * o * q^2) - 324 * m * o^2 + 1944 * n^2 * o) * r - 1944 * n * o^2 * q + 324 * o^3 * p,
    )
    roots = MVector{6, Complex{T}}(undef)
    PolynomialRoots.roots5!(roots, coeffs, eps(T), true)

    # Check roots and return maximum
    t‚ÇÅ, t‚ÇÑ = zero(T), one(T)
    k‚ÇÅ, k‚ÇÑ = k(t‚ÇÅ), k(t‚ÇÑ)
    tmax, Pmax, kmax = k‚ÇÅ > k‚ÇÑ ? (t‚ÇÅ, P‚ÇÅ, k‚ÇÅ) : (t‚ÇÑ, P‚ÇÑ, k‚ÇÑ)
    for r·µ¢ in roots
        _t, _s = real(r·µ¢), imag(r·µ¢)
        !(_s ‚âà 0) && continue # real roots only
        !(t‚ÇÅ <= _t <= t‚ÇÑ) && continue # filter roots within range
        ((_k = k(_t)) > kmax) && ((tmax, Pmax, kmax) = (_t, P(_t), _k))
    end

    return tmax, Pmax, kmax
end

function directed_angle(v‚ÇÅ::V, v‚ÇÇ::V) where {T, V <: SVector{2, T}}
    Œ± = atan(v‚ÇÅ[2], v‚ÇÅ[1]) - atan(v‚ÇÇ[2], v‚ÇÇ[1])
    return Œ± < 0 ? 2 * T(œÄ) + Œ± : Œ±
end
directed_angle(P‚±º::V, P‚Çñ::V, P‚Çó::V) where {V <: SVector{2}} = directed_angle(P‚±º - P‚Çñ, P‚Çó - P‚Çñ)

function kahan_angle(v‚ÇÅ::V, v‚ÇÇ::V) where {T, V <: SVector{2, T}}
    # Kahan's method for computing the angle between v‚ÇÅ and v‚ÇÇ.
    #   see: https://scicomp.stackexchange.com/a/27694
    a, b, c = norm(v‚ÇÅ), norm(v‚ÇÇ), norm(v‚ÇÅ - v‚ÇÇ)
    a, b = max(a, b), min(a, b)
    Œº = b ‚â• c ? c - (a - b) : (b - (a - c))
    num = ((a - b) + c) * max(Œº, zero(T))
    den = (a + (b + c)) * ((a - c) + b)
    Œ± = 2 * atan(‚àö(num / den))
    return v‚ÇÅ √ó v‚ÇÇ > 0 ? 2 * T(œÄ) - Œ± : Œ±
end
kahan_angle(P‚±º::V, P‚Çñ::V, P‚Çó::V) where {V <: SVector{2}} = kahan_angle(P‚±º - P‚Çñ, P‚Çó - P‚Çñ)
=#

####
#### GCV method for choosing the Tikhonov regularization parameter
####

struct NNLSGCVRegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W0, W1, W2}
    A::TA
    b::Tb
    m::Int
    n::Int
    Œ≥::Vector{T}
    svd_work::W0
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
end
function NNLSGCVRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    svd_work = SVDValsWorkspace(A) # workspace for computing singular values
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    Œ≥ = svd_work.S # store reference to (generalized) singular values for convenience
    return NNLSGCVRegProblem(A, b, m, n, Œ≥, svd_work, nnls_prob, nnls_prob_smooth_cache)
end

@inline solution(work::NNLSGCVRegProblem) = solution(get_cache(work.nnls_prob_smooth_cache))
@inline ncomponents(work::NNLSGCVRegProblem) = ncomponents(get_cache(work.nnls_prob_smooth_cache))

@doc raw"""
    lsqnonneg_gcv(A::AbstractMatrix, b::AbstractVector)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||L x||_2^2
```

where ``L`` is the identity matrix, and ``\mu`` is chosen via the Generalized Cross-Validation (GCV) method:

```math
\mu = \underset{\nu \ge 0}{\operatorname{argmin}}\; \frac{||AX_{\nu} - b||_2^2}{\mathcal{T}(\nu)^2}
```

where ``\mathcal{T}(\mu)`` is the "degrees of freedom" of the regularized system

```math
\mathcal{T}(\mu) = \operatorname{tr}(I - A (A^T A + \mu^2 L^T L) A^T).
```

Details of the GCV method can be found in Hansen (1992)[1].

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting increase in residual norm relative to the unregularized ``\mu = 0`` solution

# References

  1. Hansen, P.C., 1992. Analysis of Discrete Ill-Posed Problems by Means of the L-Curve. SIAM Review, 34(4), 561-580, https://doi.org/10.1137/1034115.
"""
function lsqnonneg_gcv(A::AbstractMatrix, b::AbstractVector)
    work = lsqnonneg_gcv_work(A, b)
    return lsqnonneg_gcv!(work)
end
lsqnonneg_gcv_work(A::AbstractMatrix, b::AbstractVector) = NNLSGCVRegProblem(A, b)

function lsqnonneg_gcv!(work::NNLSGCVRegProblem{T, N}; method = :brent) where {T, N}
    # Find Œº by minimizing the function G(Œº) (GCV method)
    reset_cache!(work.nnls_prob_smooth_cache)

    # Precompute singular values for GCV computation
    svdvals!(work.svd_work, work.A)

    # Non-zero lower bound for GCV to avoid log(0) in the objective function
    gcv_low = gcv_lower_bound(work)

    if method === :nlopt
        # alg = :LN_COBYLA # local, gradient-free, linear approximation of objective
        alg = :LN_BOBYQA # local, gradient-free, quadratic approximation of objective
        # alg = :GN_AGS # global, gradient-free, hilbert curve based dimension reduction
        # alg = :LN_NELDERMEAD # local, gradient-free, simplex method
        # alg = :LN_SBPLX # local, gradient-free, subspace searching simplex method
        # alg = :LD_CCSAQ # local, first-order (rough ranking: [:LD_MMA, :LD_SLSQP, :LD_LBFGS, :LD_CCSAQ, :LD_AUGLAG])
        opt               = NLopt.Opt(alg, 1)
        opt.lower_bounds  = -8.0
        opt.upper_bounds  = 2.0
        opt.xtol_abs      = 1e-4
        opt.xtol_rel      = 1e-4
        opt.ftol_abs      = 0.0
        opt.ftol_rel      = 0.0
        opt.min_objective = (logŒº, ‚àálogŒº) -> @inbounds Float64(log(max(gcv!(work, logŒº[1]), gcv_low)))
        minf, minx, ret   = NLopt.optimize(opt, [-4.0])
        logmu_final       = @inbounds T(minx[1])
    elseif method === :brent
        logmu_final, _ = brent_minimize(-8.0, 2.0; xrtol = T(0.05), xatol = T(1e-4), maxiters = 10) do logŒº
            return log(max(gcv!(work, logŒº), gcv_low))
        end
    else
        error("Unknown minimization method: $method")
    end

    # Return the final regularized solution
    mu_final = exp(logmu_final)
    x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
    x_unreg = solve!(work.nnls_prob)
    chi2_final = resnorm_sq(get_cache(work.nnls_prob_smooth_cache)) / resnorm_sq(work.nnls_prob)

    return (; x = x_final, mu = mu_final, chi2 = chi2_final)
end

# Implements equation (32) from:
#
#   Analysis of Discrete Ill-Posed Problems by Means of the L-Curve
#   Hansen et al. 1992 (https://epubs.siam.org/doi/10.1137/1034115)
#
# where here L = Id and Œª = Œº.
function gcv!(work::NNLSGCVRegProblem, logŒº)
    # Unpack buffers
    (; m, n, Œ≥) = work

    # Solve regularized NNLS problem
    Œº = exp(logŒº)
    solve!(work.nnls_prob_smooth_cache, Œº)
    cache = get_cache(work.nnls_prob_smooth_cache)

    # Compute GCV
    res¬≤ = resnorm_sq(cache) # squared residual norm ||A * x(Œº) - b||^2
    dof = gcv_dof(m, n, Œ≥, Œº) # degrees of freedom; Œ≥ are (generalized) singular values
    gcv = res¬≤ / dof^2

    return gcv
end

function gcv_and_‚àágcv!(work::NNLSGCVRegProblem, logŒº)
    # Unpack buffers
    (; m, n, Œ≥) = work

    # Solve regularized NNLS problem
    Œº = exp(logŒº)
    solve!(work.nnls_prob_smooth_cache, Œº)
    cache = get_cache(work.nnls_prob_smooth_cache)

    # Compute primal
    res¬≤ = resnorm_sq(cache) # squared residual norm ||A * x(Œº) - b||^2
    dof = gcv_dof(m, n, Œ≥, Œº) # degrees of freedom; Œ≥ are (generalized) singular values
    gcv = res¬≤ / dof^2

    # Compute derivative: ‚àÇ/‚àÇŒª [resnorm_sq(Œª) / dof(Œª)^2] = ‚àáresnorm_sq(Œª) / dof(Œª)^2 - 2 * resnorm_sq(Œª) * ‚àádof(Œª) / dof(Œª)^3
    ‚àáres¬≤ = ‚àáresnorm_sq(cache)
    ‚àádof = ‚àágcv_dof(m, n, Œ≥, Œº)
    ‚àágcv = (‚àáres¬≤ - 2 * res¬≤ * ‚àádof / dof) / dof^2

    return gcv, ‚àágcv
end

# Non-trivial lower bound of the GCV function
#   GCV(Œº) = ||A * x(Œº) - b||^2 / ùíØ(Œº)^2
# where ùíØ(Œº) is the "degrees of freedom" of the regularized system
#   ùíØ(Œº) = tr(I - A * (A'A + Œº¬≤I)‚Åª¬π * A')
#        ‚àà [max(m - n, 0), m)
# The trivial lower bound GCV(Œº) = 0 can (sometimes) be achieved when Œº = 0 if ||A * x(Œº = 0) - b|| = 0.
# Let Œµ > 0 be the RMSE threshold below which we consider the solution exact, i.e. bound ||A * x(Œº) - b|| / ‚àöm >= Œµ.
# Then, GCV(Œº) = ||A * x(Œº) - b||^2 / ùíØ(Œº)^2 >= (‚àöm * Œµ)^2 / m^2 = Œµ^2 / m
gcv_lower_bound(m::Int, n::Int, Œµ::Real) = Œµ^2 / m
gcv_lower_bound(work::NNLSGCVRegProblem{T}, Œµ::T = eps(T)) where {T} = gcv_lower_bound(work.m, work.n, Œµ)

#=
# Equivalent direct method (less efficient)
function gcv!(work::NNLSGCVRegProblem, logŒº, ::Val{extract_subproblem} = Val(false)) where {extract_subproblem}
    # Unpack buffers
    (; A, b, m, n, AŒº, A_buf, A·µÄ_buf, A·µÄA_buf) = work

    # Solve regularized NNLS problem and record residual norm ||A * x(Œº) - b||^2
    Œº = exp(logŒº)
    solve!(work.nnls_prob_smooth_cache, Œº)
    res¬≤ = resnorm_sq(get_cache(work.nnls_prob_smooth_cache))

    if extract_subproblem
        # Extract equivalent unconstrained least squares subproblem from NNLS problem
        # by extracting columns of A which correspond to nonzero components of x(Œº)
        idx = NNLS.components(get_cache(work.nnls_prob_smooth_cache).nnls_prob.nnls_work)
        n‚Ä≤ = length(idx)
        A‚Ä≤ = reshape(view(A_buf, 1:m*n‚Ä≤), m, n‚Ä≤)
        At‚Ä≤ = reshape(view(A·µÄ_buf, 1:n‚Ä≤*m), n‚Ä≤, m)
        AtA‚Ä≤ = reshape(view(A·µÄA_buf, 1:n‚Ä≤*n‚Ä≤), n‚Ä≤, n‚Ä≤)
        copyto!(A‚Ä≤, view(A, :, idx))
    else
        # Use full matrix
        A‚Ä≤ = A
        At‚Ä≤ = A·µÄ_buf
        AtA‚Ä≤ = A·µÄA_buf
    end

    # Efficient compution of
    #   AŒº = A * (A'A + Œº¬≤I)‚Åª¬π * A'
    # where the matrices have sizes
    #   A: (m, n), AŒº: (m, m), At: (n, m), AtA: (n, n)
    mul!(AtA‚Ä≤, A‚Ä≤', A‚Ä≤) # A'A
    @simd for i in 1:n
        AtA‚Ä≤[i, i] += Œº^2 # A'A + Œº¬≤I
    end
    ldiv!(At‚Ä≤, cholesky!(Symmetric(AtA‚Ä≤)), A‚Ä≤') # (A'A + Œº¬≤I)‚Åª¬π * A'
    mul!(AŒº, A‚Ä≤, At‚Ä≤) # A * (A'A + Œº¬≤I)‚Åª¬π * A'

    # Return Generalized cross-validation. See equations 27 and 32 in
    #   Hansen, P.C., 1992. Analysis of Discrete Ill-Posed Problems by Means of the L-Curve. SIAM Review, 34(4), 561-580
    #   https://doi.org/10.1137/1034115
    dof = m - tr(AŒº) # tr(I - AŒº) = m - tr(AŒº) for m x m matrix AŒº; can be considered as the "degrees of freedom" (Hansen, 1992)
    gcv = res¬≤ / dof^2 # ||A * x(Œº) - b||^2 / tr(I - AŒº)^2

    return gcv
end
=#

# Equation (27) from Hansen et al. 1992 (https://epubs.siam.org/doi/10.1137/1034115),
# specialized for L = identity:
#
#   tr(I_m - A * (A'A + Œª^2 * L'L)‚Åª¬π * A') = m - n + sum_i Œª^2 / (Œ≥_i^2 + Œª^2)
#
# where Œ≥_i are the generalized singular values, which are equivalent to ordinary
# singular values when L = identity, and size(A) = (m, n).
# Can be considered as the "degrees of freedom".
function gcv_dof(m::Int, n::Int, Œ≥::AbstractVector{T}, Œª::T) where {T}
    dof = T(max(m - n, 0)) # handle underdetermined systems (m < n)
    Œª¬≤ = abs2(Œª)
    @simd for Œ≥·µ¢ in Œ≥
        Œ≥·µ¢¬≤ = abs2(Œ≥·µ¢)
        dof += Œª¬≤ / (Œ≥·µ¢¬≤ + Œª¬≤)
    end
    return dof
end
gcv_dof(A::AbstractMatrix{T}, Œª::T) where {T} = gcv_dof(size(A)..., svdvals(A), Œª)

# DOF derivative: ‚àÇ/‚àÇŒª gcv_dof(m, n, Œ≥, Œª)
function ‚àágcv_dof(m::Int, n::Int, Œ≥::AbstractVector{T}, Œª::T) where {T}
    ‚àádof = zero(T)
    Œª¬≤ = abs2(Œª)
    @simd for Œ≥·µ¢ in Œ≥
        Œ≥·µ¢¬≤ = abs2(Œ≥·µ¢)
        ‚àádof += 2 * Œª * Œ≥·µ¢¬≤ / (Œ≥·µ¢¬≤ + Œª¬≤)^2
    end
    return ‚àádof
end
‚àágcv_dof(A::AbstractMatrix{T}, Œª::T) where {T} = ‚àágcv_dof(size(A)..., svdvals(A), Œª)
