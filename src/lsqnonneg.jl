####
#### Unregularized NNLS problem
####

struct NNLSProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_work::W
end
function NNLSProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_work = NNLS.NNLSWorkspace(A, b)
    return NNLSProblem(A, b, m, n, nnls_work)
end

# Solve NNLS problem
solve!(work::NNLSProblem, A = work.A, b = work.b) = NNLS.nnls!(work.nnls_work, A, b)

@inline solution(work::NNLSProblem) = NNLS.solution(work.nnls_work)
@inline ncomponents(work::NNLSProblem) = NNLS.ncomponents(work.nnls_work)
@inline resnorm(work::NNLSProblem) = NNLS.residualnorm(work.nnls_work)
@inline resnorm_sq(work::NNLSProblem) = resnorm(work)^2

@doc raw"""
    lsqnonneg(A::AbstractMatrix, b::AbstractVector)

Compute the nonnegative least-squares (NNLS) solution ``X`` of the problem:

```math
X = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2.
```

# Arguments

  - `A::AbstractMatrix`: Left hand side matrix acting on `x`
  - `b::AbstractVector`: Right hand side vector

# Outputs

  - `X::AbstractVector`: NNLS solution
"""
lsqnonneg(A::AbstractMatrix, b::AbstractVector) = lsqnonneg!(lsqnonneg_work(A, b))
lsqnonneg_work(A::AbstractMatrix, b::AbstractVector) = NNLSProblem(A, b)
lsqnonneg!(work::NNLSProblem) = solve!(work)
lsqnonneg!(work::NNLSProblem{T}, A::AbstractMatrix{T}, b::AbstractVector{T}) where {T} = solve!(work, A, b)

####
#### Lazy wrappers for LHS matrix and RHS vector for augmented Tikhonov-regularized NNLS problems
####

struct PaddedVector{T, Tb <: AbstractVector{T}} <: AbstractVector{T}
    b::Tb
    pad::Int
end
Base.size(x::PaddedVector) = (length(x.b) + x.pad,)
Base.parent(x::PaddedVector) = x.b

function Base.copyto!(y::AbstractVector{T}, x::PaddedVector{T}) where {T}
    @assert size(x) == size(y)
    (; b, pad) = x
    m = length(b)
    @simd for i in 1:m
        y[i] = b[i]
    end
    @simd for i in m+1:m+pad
        y[i] = zero(T)
    end
    return y
end

struct TikhonovPaddedMatrix{T, TA <: AbstractMatrix{T}} <: AbstractMatrix{T}
    A::TA
    Î¼::Base.RefValue{T}
end
TikhonovPaddedMatrix(A::AbstractMatrix, Î¼::Real) = TikhonovPaddedMatrix(A, Ref(Î¼))
Base.size(P::TikhonovPaddedMatrix) = ((m, n) = size(P.A); return (m + n, n))
Base.parent(P::TikhonovPaddedMatrix) = P.A
mu(P::TikhonovPaddedMatrix) = P.Î¼[]
mu!(P::TikhonovPaddedMatrix, Î¼::Real) = P.Î¼[] = Î¼

function Base.copyto!(B::AbstractMatrix{T}, P::TikhonovPaddedMatrix{T}) where {T}
    @assert size(P) == size(B)
    A, Î¼ = parent(P), mu(P)
    m, n = size(A)
    @inbounds for j in 1:n
        @simd for i in 1:m
            B[i, j] = A[i, j]
        end
        @simd for i in m+1:m+n
            B[i, j] = ifelse(i == m + j, Î¼, zero(T))
        end
    end
    return B
end

####
#### Tikhonov regularized NNLS problem
####

struct NNLSTikhonovRegProblem{
    T,
    TA <: AbstractMatrix{T},
    Tb <: AbstractVector{T},
    W <: NNLSProblem{T, TikhonovPaddedMatrix{T, TA}, PaddedVector{T, Tb}},
    B,
}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W
    buffers::B
end
function NNLSTikhonovRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}, Î¼::Real = T(NaN)) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(TikhonovPaddedMatrix(A, Î¼), PaddedVector(b, n))
    buffers = (x = zeros(T, n), y = zeros(T, n))
    return NNLSTikhonovRegProblem(A, b, m, n, nnls_prob, buffers)
end

@doc raw"""
    lsqnonneg_tikh(A::AbstractMatrix, b::AbstractVector, Î¼::Real)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||x||_2^2.
```

# Arguments

  - `A::AbstractMatrix`: Left hand side matrix acting on `x`
  - `b::AbstractVector`: Right hand side vector
  - `Î¼::Real`: Regularization parameter

# Outputs

  - `X::AbstractVector`: NNLS solution
"""
lsqnonneg_tikh(A::AbstractMatrix, b::AbstractVector, Î¼::Real) = lsqnonneg_tikh!(lsqnonneg_tikh_work(A, b), Î¼)
lsqnonneg_tikh_work(A::AbstractMatrix, b::AbstractVector) = NNLSTikhonovRegProblem(A, b)
lsqnonneg_tikh!(work::NNLSTikhonovRegProblem, Î¼::Real) = solve!(work, Î¼)

mu(work::NNLSTikhonovRegProblem) = mu(work.nnls_prob.A)
mu!(work::NNLSTikhonovRegProblem, Î¼::Real) = mu!(work.nnls_prob.A, Î¼)

function solve!(work::NNLSTikhonovRegProblem, Î¼::Real)
    # Set regularization parameter and solve NNLS problem
    mu!(work, Î¼)
    solve!(work.nnls_prob)
    return solution(work)
end

@inline solution(work::NNLSTikhonovRegProblem) = NNLS.solution(work.nnls_prob.nnls_work)
@inline ncomponents(work::NNLSTikhonovRegProblem) = NNLS.ncomponents(work.nnls_prob.nnls_work)

@inline loss(work::NNLSTikhonovRegProblem) = NNLS.residualnorm(work.nnls_prob.nnls_work)^2

regnorm(work::NNLSTikhonovRegProblem) = mu(work)^2 * seminorm_sq(work) # Î¼Â²||x||Â²
âˆ‡regnorm(work::NNLSTikhonovRegProblem) = 2 * mu(work) * seminorm_sq(work) + mu(work)^2 * âˆ‡seminorm_sq(work) # d/dÎ¼ [Î¼Â²||x||Â²] = 2Î¼||x||Â² + Î¼Â² d/dÎ¼ [||x||Â²]

resnorm(work::NNLSTikhonovRegProblem) = âˆš(resnorm_sq(work)) # ||Ax-b||
resnorm_sq(work::NNLSTikhonovRegProblem) = max(loss(work) - regnorm(work), 0) # ||Ax-b||Â²
âˆ‡resnorm_sq(work::NNLSTikhonovRegProblem, âˆ‡ = gradient_temps(work)) = 4 * âˆ‡.Î¼^3 * âˆ‡.xáµ€Bâ»Â¹x # d/dÎ¼ [||Ax-b||Â²]
âˆ‡Â²resnorm_sq(work::NNLSTikhonovRegProblem, âˆ‡Â² = hessian_temps(work)) = 12 * âˆ‡Â².Î¼^2 * âˆ‡Â².xáµ€Bâ»Â¹x - 24 * âˆ‡Â².Î¼^4 * âˆ‡Â².xáµ€Bâ»áµ€Bâ»Â¹x # dÂ²/dÎ¼Â² [||Ax-b||Â²]

seminorm(work::NNLSTikhonovRegProblem) = âˆš(seminorm_sq(work)) # ||x||
seminorm_sq(work::NNLSTikhonovRegProblem) = GC.@preserve work sum(abs2, NNLS.positive_solution(work.nnls_prob.nnls_work)) # ||x||Â²
âˆ‡seminorm_sq(work::NNLSTikhonovRegProblem, âˆ‡ = gradient_temps(work)) = -4 * âˆ‡.Î¼ * âˆ‡.xáµ€Bâ»Â¹x # d/dÎ¼ [||x||Â²]
âˆ‡Â²seminorm_sq(work::NNLSTikhonovRegProblem, âˆ‡Â² = hessian_temps(work)) = -4 * âˆ‡Â².xáµ€Bâ»Â¹x + 24 * âˆ‡Â².Î¼^2 * âˆ‡Â².xáµ€Bâ»áµ€Bâ»Â¹x # dÂ²/dÎ¼Â² [||x||Â²]

solution_gradnorm(work::NNLSTikhonovRegProblem, âˆ‡Â² = hessian_temps(work)) = âˆš(solution_gradnorm_sq(work, âˆ‡Â²)) # ||dx/dÎ¼|| = ||-2Î¼ * Bâ»Â¹x|| = 2Î¼ * ||Bâ»Â¹x||
solution_gradnorm_sq(work::NNLSTikhonovRegProblem, âˆ‡Â² = hessian_temps(work)) = 4 * âˆ‡Â².Î¼^2 * âˆ‡Â².xáµ€Bâ»áµ€Bâ»Â¹x # ||dx/dÎ¼||Â² = ||-2Î¼ * Bâ»Â¹x||Â² = 4Î¼Â² * xáµ€Bâ»áµ€Bâ»Â¹x

# L-curve: (Î¾(Î¼), Î·(Î¼)) = (||Ax-b||^2, ||x||^2)
curvature(::typeof(identity), work::NNLSTikhonovRegProblem, âˆ‡ = gradient_temps(work)) = inv(2 * âˆ‡.xáµ€Bâ»Â¹x * âˆš(1 + âˆ‡.Î¼^4)^3)

# L-curve: (Î¾Ì„(Î¼), Î·Ì„(Î¼)) = (log||Ax-b||^2, log||x||^2)
function curvature(::typeof(log), work::NNLSTikhonovRegProblem, âˆ‡ = gradient_temps(work))
    â„“Â² = loss(work) # â„“Â² = ||Ax-b||^2 + Î¼Â²||x||^2 = Î¾Â² + Î¼Â²Î·Â²
    Î¾Â² = resnorm_sq(work)
    Î·Â² = seminorm_sq(work)
    Î¾â´, Î·â´ = Î¾Â²^2, Î·Â²^2
    CÌ„ = Î¾Â² * Î·Â² * (Î¾Â² * Î·Â² - (2 * âˆ‡.xáµ€Bâ»Â¹x) * âˆ‡.Î¼^2 * â„“Â²) / (2 * âˆ‡.xáµ€Bâ»Â¹x * âˆš(Î¾â´ + âˆ‡.Î¼^4 * Î·â´)^3)
    return CÌ„
end

function gradient_temps(work::NNLSTikhonovRegProblem{T}) where {T}
    GC.@preserve work begin
        (; nnls_work) = work.nnls_prob
        B = cholesky!(NNLS.NormalEquation(), nnls_work) # B = A'A + Î¼Â²I = U'U
        xâ‚Š = NNLS.positive_solution(nnls_work)
        tmp = uview(work.buffers.y, 1:length(xâ‚Š))

        Î¼ = mu(work)
        copyto!(tmp, xâ‚Š)
        NNLS.solve_triangular_system!(tmp, B, Val(true)) # tmp = U'\x
        xáµ€Bâ»Â¹x = sum(abs2, tmp) # x'B\x = x'(U'U)\x = ||U'\x||^2

        return (; Î¼, xáµ€Bâ»Â¹x)
    end
end

function hessian_temps(work::NNLSTikhonovRegProblem{T}) where {T}
    GC.@preserve work begin
        (; nnls_work) = work.nnls_prob
        B = cholesky!(NNLS.NormalEquation(), nnls_work) # B = A'A + Î¼Â²I = U'U
        xâ‚Š = NNLS.positive_solution(nnls_work)
        tmp = uview(work.buffers.y, 1:length(xâ‚Š))

        Î¼ = mu(work)
        copyto!(tmp, xâ‚Š)
        NNLS.solve_triangular_system!(tmp, B, Val(true)) # tmp = U'\x
        xáµ€Bâ»Â¹x = sum(abs2, tmp) # x'B\x = x'(U'U)\x = ||U'\x||^2

        NNLS.solve_triangular_system!(tmp, B, Val(false)) # tmp = U\(U'\x) = (U'U)\x
        xáµ€Bâ»áµ€Bâ»Â¹x = sum(abs2, tmp) # x'B'\B\x = ||B\x||^2 = ||(U'U)\x||^2

        return (; Î¼, xáµ€Bâ»Â¹x, xáµ€Bâ»áµ€Bâ»Â¹x)
    end
end

function chi2_relerr!(work::NNLSTikhonovRegProblem, Ï‡Â²target, logÎ¼, âˆ‡logÎ¼ = nothing)
    # NOTE: assumes `solve!(work, Î¼)` has been called and that the solution is ready
    Î¼ = exp(logÎ¼)
    resÂ² = resnorm_sq(work)
    relerr = log(resÂ² / Ï‡Â²target) # better behaved than resÂ² / Ï‡Â²target - 1 for large resÂ²?
    if âˆ‡logÎ¼ !== nothing && length(âˆ‡logÎ¼) > 0
        âˆ‚resÂ²_âˆ‚Î¼ = âˆ‡resnorm_sq(work)
        âˆ‚relerr_âˆ‚logÎ¼ = Î¼ * âˆ‚resÂ²_âˆ‚Î¼ / resÂ²
        @inbounds âˆ‡logÎ¼[1] = âˆ‚relerr_âˆ‚logÎ¼
    end
    return relerr
end
chi2_relerrâ»Â¹(Ï‡Â²target, relerr) = Ï‡Â²target * exp(relerr)

# Helper struct which wraps `N` caches of `NNLSTikhonovRegProblem` workspaces.
# Useful for optimization problems where the last function call may not be
# the optimium, but perhaps it was one or two calls previous and is still in the
# `NNLSTikhonovRegProblemCache` and a recomputation can be avoided.
struct NNLSTikhonovRegProblemCache{N, W <: NTuple{N}}
    cache::W
    idx::Base.RefValue{Int}
end
function NNLSTikhonovRegProblemCache(A::AbstractMatrix{T}, b::AbstractVector{T}, ::Val{N} = Val(5)) where {T, N}
    cache = ntuple(_ -> NNLSTikhonovRegProblem(A, b), N)
    idx = Ref(1)
    return NNLSTikhonovRegProblemCache(cache, idx)
end
increment_cache_index!(work::NNLSTikhonovRegProblemCache{N}) where {N} = (work.idx[] = mod1(work.idx[] + 1, N))
set_cache_index!(work::NNLSTikhonovRegProblemCache{N}, i) where {N} = (work.idx[] = mod1(i, N))
reset_cache!(work::NNLSTikhonovRegProblemCache{N}) where {N} = foreach(w -> mu!(w, NaN), work.cache)
Base.getindex(work::NNLSTikhonovRegProblemCache) = work.cache[work.idx[]]

function solve!(work::NNLSTikhonovRegProblemCache, Î¼::Real)
    i = findfirst(w -> Î¼ == mu(w), work.cache)
    if i === nothing
        increment_cache_index!(work)
        solve!(work[], Î¼)
    else
        set_cache_index!(work, i)
    end
    return solution(work[])
end

####
#### Chi2 method for choosing the Tikhonov regularization parameter
####

struct NNLSChi2RegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W1, W2}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
end
function NNLSChi2RegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    return NNLSChi2RegProblem(A, b, m, n, nnls_prob, nnls_prob_smooth_cache)
end

@inline solution(work::NNLSChi2RegProblem) = solution(work.nnls_prob_smooth_cache[])
@inline ncomponents(work::NNLSChi2RegProblem) = ncomponents(work.nnls_prob_smooth_cache[])

@doc raw"""
    lsqnonneg_chi2(A::AbstractMatrix, b::AbstractVector, chi2_target::Real)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||x||_2^2
```

where ``\mu`` is determined by solving:

```math
\chi^2(\mu) = \frac{||AX_{\mu} - b||_2^2}{||AX_{0} - b||_2^2} = \chi^2_{\mathrm{target}}.
```

That is, ``\mu`` is chosen such that the squared residual norm of the regularized problem is `chi2_target`
times larger than the squared residual norm of the unregularized problem.

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data
  - `chi2_target::Real`: Target ``\chi^2(\mu)``; typically a small value, e.g. 1.02 representing a 2% increase

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting ``\chi^2(\mu)``, which should be approximately equal to `chi2_target`
"""
function lsqnonneg_chi2(A::AbstractMatrix, b::AbstractVector, chi2_target::Real, args...; kwargs...)
    work = lsqnonneg_chi2_work(A, b)
    return lsqnonneg_chi2!(work, chi2_target, args...; kwargs...)
end
lsqnonneg_chi2_work(A::AbstractMatrix, b::AbstractVector) = NNLSChi2RegProblem(A, b)

function lsqnonneg_chi2!(work::NNLSChi2RegProblem{T}, chi2_target::T, legacy::Bool = false; method::Symbol = legacy ? :legacy : :bisect) where {T}
    # Non-regularized solution
    solve!(work.nnls_prob)
    x_unreg = solution(work.nnls_prob)
    resÂ²_min = resnorm_sq(work.nnls_prob)

    if resÂ²_min == 0 || ncomponents(work.nnls_prob) == 0
        # 1. If non-regularized solution is exact, the only solution to resÂ²(Î¼) = chi2_target * resÂ²_min = 0 is Î¼ = 0, since resÂ²(Î¼) > 0 for all Î¼ > 0.
        # 2. If non-regularized solution is zero, any value of Î¼ > 0 also results in x(Î¼) = 0, and so resÂ²(Î¼) = chi2_target * resÂ²_min has either no solutions if chi2_target > 1, or infinitely many solutions if chi2_target = 1; choose Î¼ = 0 and chi2_target = 1.
        x_final = x_unreg
        return (; x = x_final, mu = zero(T), chi2 = one(T))
    end

    # Prepare to solve
    resÂ²_target = chi2_target * resÂ²_min
    reset_cache!(work.nnls_prob_smooth_cache)

    if method === :legacy
        # Use the legacy algorithm: double Î¼ starting from an initial guess, then interpolate the root using a cubic spline fit
        mu_final, resÂ²_final = chi2_search_from_minimum(resÂ²_min, chi2_target; legacy) do Î¼
            Î¼ == 0 && return resÂ²_min
            solve!(work.nnls_prob_smooth_cache, Î¼)
            return resnorm_sq(work.nnls_prob_smooth_cache[])
        end
        if mu_final == 0
            x_final = x_unreg
        else
            x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
        end

    elseif method === :bisect
        f = function (logÎ¼)
            solve!(work.nnls_prob_smooth_cache, exp(logÎ¼))
            return chi2_relerr!(work.nnls_prob_smooth_cache[], resÂ²_target, logÎ¼)
        end

        # Find bracketing interval containing root, then perform bisection search with slightly higher tolerance to not waste f evals
        a, b, fa, fb = bracket_root_monotonic(f, T(-4.0), T(1.0); dilate = T(1.5), mono = +1, maxiters = 6)

        if fa * fb < 0
            # Bracketing interval found
            a, fa, c, fc, b, fb = bisect_root(f, a, b, fa, fb; xatol = T(0.05), xrtol = T(0.0), ftol = T(1e-2) * (chi2_target - 1), maxiters = 100)

            # Root of secant line through `(a, fa), (b, fb)` or `(c, fc), (b, fb)` to improve bisection accuracy
            tmp = fa * fc < 0 ? root_real_linear(a, c, fa, fc) : fc * fb < 0 ? root_real_linear(c, b, fc, fb) : T(NaN)
            d, fd = isnan(tmp) ? (c, fc) : (tmp, f(tmp))

            # Return regularization parameter with lowest abs(relerr)
            logmu_final, relerr_final = abs(fd) < abs(fc) ? (d, fd) : (c, fc)
        else
            # No bracketing interval found; choose point with smallest value of f (note: this branch should never be reached)
            logmu_final, relerr_final = !isfinite(fa) ? (b, fb) : !isfinite(fb) ? (a, fa) : abs(fa) < abs(fb) ? (a, fa) : (b, fb)
        end

        if isfinite(relerr_final)
            mu_final, resÂ²_final = exp(logmu_final), chi2_relerrâ»Â¹(resÂ²_target, relerr_final)
            x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
        else
            x_final, mu_final, resÂ²_final = x_unreg, zero(T), one(T)
        end

    elseif method === :brent
        f = function (logÎ¼)
            solve!(work.nnls_prob_smooth_cache, exp(logÎ¼))
            return chi2_relerr!(work.nnls_prob_smooth_cache[], resÂ²_target, logÎ¼)
        end

        # Find bracketing interval containing root
        a, b, fa, fb = bracket_root_monotonic(f, T(-4.0), T(1.0); dilate = T(1.5), mono = +1, maxiters = 100)

        if fa * fb < 0
            # Find root using Brent's method
            logmu_final, relerr_final = brent_root(f, a, b, fa, fb; xatol = T(0.0), xrtol = T(0.0), ftol = T(1e-3) * (chi2_target - 1), maxiters = 100)
        else
            # No bracketing interval found; choose point with smallest value of f (note: this branch should never be reached)
            logmu_final, relerr_final = !isfinite(fa) ? (b, fb) : !isfinite(fb) ? (a, fa) : abs(fa) < abs(fb) ? (a, fa) : (b, fb)
        end

        if isfinite(relerr_final)
            mu_final, resÂ²_final = exp(logmu_final), chi2_relerrâ»Â¹(resÂ²_target, relerr_final)
            x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
        else
            x_final, mu_final, resÂ²_final = x_unreg, zero(T), one(T)
        end
    else
        error("Unknown root-finding method: :$method")
    end

    return (; x = x_final, mu = mu_final, chi2 = resÂ²_final / resÂ²_min)
end

function chi2_search_from_minimum(f, resÂ²min::T, Ï‡Â²fact::T, Î¼min::T = T(1e-3), Î¼fact = T(2.0); legacy = false) where {T}
    # Minimize energy of spectrum; loop to find largest Î¼ that keeps chi-squared in desired range
    Î¼_cache = T[zero(T)]
    resÂ²_cache = T[resÂ²min]
    Î¼new = Î¼min
    while true
        # Cache function value at Î¼ = Î¼new
        resÂ²new = f(Î¼new)
        push!(Î¼_cache, Î¼new)
        push!(resÂ²_cache, resÂ²new)

        # Break when Ï‡Â²fact reached, else increase regularization
        (resÂ²new >= Ï‡Â²fact * resÂ²min) && break
        Î¼new *= Î¼fact
    end

    # Solve resÂ²(Î¼) = Ï‡Â²fact * resÂ²min using a spline fitting root finding method
    if legacy
        # Legacy algorithm fits spline to all (Î¼, resÂ²) values observed, including for Î¼=0.
        # This poses several problems:
        #   1) while unlikely, it is possible for the spline to return a negative regularization parameter
        #   2) the Î¼ values are exponentially spaced, leading to poorly conditioned splines
        Î¼ = spline_root_legacy(Î¼_cache, resÂ²_cache, Ï‡Â²fact * resÂ²min)
    else
        if length(Î¼_cache) == 2
            # Solution is contained in [0,Î¼min]; `spline_root` with two points performs root finding via simple linear interpolation
            Î¼ = spline_root(Î¼_cache, resÂ²_cache, Ï‡Â²fact * resÂ²min; deg_spline = 1)
            Î¼ = isnan(Î¼) ? Î¼min : Î¼
        else
            # Perform spline fit on log-log scale on data with Î¼ > 0. This solves the above problems with the legacy algorithm:
            #   1) Root is found in terms of logÎ¼, guaranteeing Î¼ > 0
            #   2) logÎ¼ is linearly spaced, leading to well-conditioned splines
            logÎ¼ = spline_root(log.(Î¼_cache[2:end]), log.(resÂ²_cache[2:end]), log(Ï‡Â²fact * resÂ²min); deg_spline = 1)
            Î¼ = isnan(logÎ¼) ? Î¼min : exp(logÎ¼)
        end
    end

    # Compute the final regularized solution
    resÂ² = f(Î¼)

    return Î¼, resÂ²
end

####
#### Morozov discrepency principle (MDP) method for choosing the Tikhonov regularization parameter
####

struct NNLSMDPRegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W1, W2}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
end
function NNLSMDPRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    return NNLSMDPRegProblem(A, b, m, n, nnls_prob, nnls_prob_smooth_cache)
end

@inline solution(work::NNLSMDPRegProblem) = solution(work.nnls_prob_smooth_cache[])
@inline ncomponents(work::NNLSMDPRegProblem) = ncomponents(work.nnls_prob_smooth_cache[])

@doc raw"""
    lsqnonneg_mdp(A::AbstractMatrix, b::AbstractVector, Î´::Real)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||x||_2^2
```

where ``\mu`` is chosen using Morozov's Discrepency Principle (MDP)[1,2]:

```math
\mu = \operatorname{sup}\; \left\{ \nu \ge 0 : ||AX_{\nu} - b|| \le \delta \right\}.
```

That is, ``\mu`` is maximized subject to the constraint that the residual norm of the regularized problem is at most ``\delta``[1].

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data
  - `Î´::Real`: Upper bound on regularized residual norm

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting increase in residual norm relative to the unregularized ``\mu = 0`` solution

# References

  1. Morozov VA. Methods for Solving Incorrectly Posed Problems. Springer Science & Business Media, 2012.
  2. Clason C, Kaltenbacher B, Resmerita E. Regularization of Ill-Posed Problems with Non-negative Solutions. In: Bauschke HH, Burachik RS, Luke DR (eds) Splitting Algorithms, Modern Operator Theory, and Applications. Cham: Springer International Publishing, pp. 113â€“135.
"""
function lsqnonneg_mdp(A::AbstractMatrix, b::AbstractVector, Î´::Real, args...; kwargs...)
    work = lsqnonneg_mdp_work(A, b)
    return lsqnonneg_mdp!(work, Î´, args...; kwargs...)
end
lsqnonneg_mdp_work(A::AbstractMatrix, b::AbstractVector) = NNLSMDPRegProblem(A, b)

function lsqnonneg_mdp!(work::NNLSMDPRegProblem{T}, Î´::T) where {T}
    # Non-regularized solution
    solve!(work.nnls_prob)
    x_unreg = solution(work.nnls_prob)
    resÂ²_min = resnorm_sq(work.nnls_prob)

    #TODO: throw error if Î´ > ||b||, since ||A * x(Î¼) - b|| <= ||b|| when A, x, b are componentwise nonnegative, or return... what?
    # if Î´ >= norm(work.nnls_prob.b)
    #     error("Î´ = $Î´ is greater than the norm of the data vector ||b|| = $(norm(work.nnls_prob.b))")
    # end
    if Î´ <= resÂ²_min
        x_final = x_unreg
        return (; x = x_final, mu = zero(T), chi2 = one(T))
    end

    # Prepare to solve
    reset_cache!(work.nnls_prob_smooth_cache)

    function f(logÎ¼)
        solve!(work.nnls_prob_smooth_cache, exp(logÎ¼))
        return resnorm_sq(work.nnls_prob_smooth_cache[]) - Î´^2
    end

    # Find bracketing interval containing root
    a, b, fa, fb = bracket_root_monotonic(f, T(-4.0), T(1.0); dilate = T(1.5), mono = +1, maxiters = 100)

    if fa * fb < 0
        # Find root using Brent's method
        logmu_final, err_final = brent_root(f, a, b, fa, fb; xatol = T(0.0), xrtol = T(0.0), ftol = T(1e-3) * Î´^2, maxiters = 100)
    else
        # No bracketing interval found; choose point with smallest value of f (note: this branch should never be reached)
        logmu_final, err_final = !isfinite(fa) ? (b, fb) : !isfinite(fb) ? (a, fa) : abs(fa) < abs(fb) ? (a, fa) : (b, fb)
    end

    if isfinite(err_final)
        mu_final, resÂ²_final = exp(logmu_final), Î´^2 + err_final
        x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
    else
        x_final, mu_final, resÂ²_final = x_unreg, zero(T), one(T)
    end

    return (; x = x_final, mu = mu_final, chi2 = resÂ²_final / resÂ²_min)
end

####
#### L-curve method for choosing the Tikhonov regularization parameter
####

struct NNLSLCurveRegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W1, W2, C1, C2}
    A::TA
    b::Tb
    m::Int
    n::Int
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
    lsqnonneg_lcurve_fun_cache::C1
    lcurve_corner_caches::C2
end
function NNLSLCurveRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    lsqnonneg_lcurve_fun_cache = GrowableCache{T, SVector{2, T}}(64, isapprox)
    lcurve_corner_caches = (
        GrowableCache{T, LCurveCornerPoint{T}}(64, isapprox),
        GrowableCache{T, LCurveCornerState{T}}(64, isapprox),
    )
    return NNLSLCurveRegProblem(A, b, m, n, nnls_prob, nnls_prob_smooth_cache, lsqnonneg_lcurve_fun_cache, lcurve_corner_caches)
end

@inline solution(work::NNLSLCurveRegProblem) = solution(work.nnls_prob_smooth_cache[])
@inline ncomponents(work::NNLSLCurveRegProblem) = ncomponents(work.nnls_prob_smooth_cache[])

@doc raw"""
    lsqnonneg_lcurve(A::AbstractMatrix, b::AbstractVector)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||L x||_2^2
```

where ``L`` is the identity matrix, and ``\mu`` is chosen by locating the corner of the "L-curve"[1].
Details of L-curve theory can be found in Hansen (1992)[2].

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting increase in residual norm relative to the unregularized ``\mu = 0`` solution

# References

  1. A. Cultrera and L. Callegaro, "A simple algorithm to find the L-curve corner in the regularization of ill-posed inverse problems". IOPSciNotes, vol. 1, no. 2, p. 025004, Aug. 2020, https://doi.org/10.1088/2633-1357/abad0d.
  2. Hansen, P.C., 1992. Analysis of Discrete Ill-Posed Problems by Means of the L-Curve. SIAM Review, 34(4), 561-580, https://doi.org/10.1137/1034115.
"""
function lsqnonneg_lcurve(A::AbstractMatrix, b::AbstractVector)
    work = lsqnonneg_lcurve_work(A, b)
    return lsqnonneg_lcurve!(work)
end
lsqnonneg_lcurve_work(A::AbstractMatrix, b::AbstractVector) = NNLSLCurveRegProblem(A, b)

function lsqnonneg_lcurve!(work::NNLSLCurveRegProblem{T}) where {T}
    # Compute the regularization using the L-curve method
    reset_cache!(work.nnls_prob_smooth_cache)

    # A point on the L-curve is given by (Î¾(Î¼), Î·(Î¼)) = (log||Ax-b||^2, log||x||^2)
    #   Note: Squaring the norms is convenient for computing gradients of (Î¾(Î¼), Î·(Î¼));
    #         this scales the L-curve, but does not change Î¼* = argmax C(Î¾(Î¼), Î·(Î¼)).
    function f_lcurve(logÎ¼)
        solve!(work.nnls_prob_smooth_cache, exp(logÎ¼))
        Î¾ = log(resnorm_sq(work.nnls_prob_smooth_cache[]))
        Î· = log(seminorm_sq(work.nnls_prob_smooth_cache[]))
        return SA{T}[Î¾, Î·]
    end

    # Build cached function and solve
    f_lcurve_cached = CachedFunction(f_lcurve, empty!(work.lsqnonneg_lcurve_fun_cache))
    f = LCurveCornerCachedFunction(f_lcurve_cached, empty!.(work.lcurve_corner_caches)...)

    logmu_bounds = (T(-8), T(2))
    logmu_final = lcurve_corner(f, logmu_bounds...)

    # Return the final regularized solution
    mu_final = exp(logmu_final)
    x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
    x_unreg = solve!(work.nnls_prob)
    chi2_final = resnorm_sq(work.nnls_prob_smooth_cache[]) / resnorm_sq(work.nnls_prob)

    return (; x = x_final, mu = mu_final, chi2 = chi2_final)
end

struct LCurveCornerState{T}
    xâƒ—::SVector{4, T} # grid of regularization parameters
    Pâƒ—::SVector{4, SVector{2, T}} # points (residual norm, solution seminorm) evaluated at xâƒ—
end
@inline Base.iterate(s::LCurveCornerState, args...) = iterate((s.xâƒ—, s.Pâƒ—), args...)

struct LCurveCornerPoint{T}
    P::SVector{2, T} # grid point
    C::T # curvature
end
LCurveCornerPoint(P::SVector{2, T}) where {T} = LCurveCornerPoint(P, T(-Inf))
@inline Base.iterate(p::LCurveCornerPoint, args...) = iterate((p.P, p.C), args...)

struct LCurveCornerCachedFunction{T, F <: CachedFunction{T, SVector{2, T}}, C1 <: GrowableCache{T, LCurveCornerPoint{T}}, C2 <: GrowableCache{T, LCurveCornerState{T}}}
    f::F
    point_cache::C1
    state_cache::C2
end
@inline Base.empty!(f::LCurveCornerCachedFunction) = (empty!(f.f); empty!(f.point_cache); empty!(f.state_cache); f)
@inline (f::LCurveCornerCachedFunction{T})(x::T) where {T} = f.f(x)

@doc raw"""
    lcurve_corner(f, xlow, xhigh)

Find the corner of the L-curve via curvature maximization using a modified version of Algorithm 1 from Cultrera and Callegaro (2020)[1].

# References

  1. A. Cultrera and L. Callegaro, "A simple algorithm to find the L-curve corner in the regularization of ill-posed inverse problems". IOPSciNotes, vol. 1, no. 2, p. 025004, Aug. 2020, https://doi.org/10.1088/2633-1357/abad0d.
"""
function lcurve_corner(f::LCurveCornerCachedFunction{T}, xlow::T = -8.0, xhigh::T = 2.0; xtol = 0.05, Ptol = 0.05, Ctol = 0.01, backtracking = true) where {T}
    # Initialize state
    state = initial_state(f, T(xlow), T(xhigh))

    # Tolerances are relative to initial curve size
    Ptopleft, Pbottomright = state.Pâƒ—[1], state.Pâƒ—[4]
    Pdiam = norm(Ptopleft - Pbottomright)
    Ptol = Pdiam * T(Ptol) # convergence occurs when diameter of L-curve state is less than Ptol
    Ctol = Pdiam * T(Ctol) # note: *not* a tolerance on curvature, but on the minimum diameter of the L-curve state used to estimate curvature (see `Pfilter` below)

    # For very small regularization points on the L-curve may be extremely close, leading to
    # numerically unstable curvature estimates. Assign these points -Inf curvature.
    Pfilter = P -> min(norm(P - Ptopleft), norm(P - Pbottomright)) > T(Ctol)
    update_curvature!(f, state, Pfilter)

    # msg(s, state) = (@info "$s: [xâƒ—, Pâƒ—, Câƒ—] = "; display(hcat(state.xâƒ—, state.Pâƒ—, [f.point_cache[x].C for x in state.xâƒ—])))
    # msg("Starting", state)

    iter = 0
    while true
        iter += 1
        if backtracking
            # Find state with minimum diameter which contains the current best estimate maximum curvature point
            (x, (P, C)), _, _ = mapfindmax(T, ((x, (P, C)),) -> C, pairs(f.point_cache))
            for (_, s) in f.state_cache
                if (s.xâƒ—[2] == x || s.xâƒ—[3] == x) && abs(s.xâƒ—[4] - s.xâƒ—[1]) <= abs(state.xâƒ—[4] - state.xâƒ—[1])
                    state = s
                end
            end
        end

        # Move state toward region of lower curvature
        if f.point_cache[state.xâƒ—[2]].C > f.point_cache[state.xâƒ—[3]].C
            state = move_left(f, state)
            update_curvature!(f, state, Pfilter)
            # msg("Câ‚‚ > Câ‚ƒ; moved left", state)
        else
            state = move_right(f, state)
            update_curvature!(f, state, Pfilter)
            # msg("Câ‚ƒ â‰¥ Câ‚‚; moved right", state)
        end
        backtracking && push!(f.state_cache, (iter, state))
        is_converged(state; xtol = T(xtol), Ptol = T(Ptol)) && break
    end

    x = f.point_cache[state.xâƒ—[2]].C > f.point_cache[state.xâƒ—[3]].C ? state.xâƒ—[2] : state.xâƒ—[3]
    # msg("Converged", state)

    return x
end

function initial_state(f::LCurveCornerCachedFunction{T}, xâ‚::T, xâ‚„::T) where {T}
    Ï† = T(Base.MathConstants.Ï†)
    xâ‚‚ = (Ï† * xâ‚ + xâ‚„) / (Ï† + 1)
    xâ‚ƒ = xâ‚ + (xâ‚„ - xâ‚‚)
    xâƒ— = SA[xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„]
    Pâƒ— = SA[f(xâ‚), f(xâ‚‚), f(xâ‚ƒ), f(xâ‚„)]
    Base.Cartesian.@nexprs 4 i -> push!(f.point_cache, (xâƒ—[i], LCurveCornerPoint(Pâƒ—[i])))
    return LCurveCornerState(xâƒ—, Pâƒ—)
end

is_converged(state::LCurveCornerState; xtol, Ptol) = abs(state.xâƒ—[4] - state.xâƒ—[1]) < xtol || norm(state.Pâƒ—[1] - state.Pâƒ—[4]) < Ptol

function move_left(f::LCurveCornerCachedFunction{T}, state::LCurveCornerState{T}) where {T}
    (; xâƒ—, Pâƒ—) = state
    Ï† = T(Base.MathConstants.Ï†)
    xâƒ— = SA[xâƒ—[1], (Ï†*xâƒ—[1]+xâƒ—[3])/(Ï†+1), xâƒ—[2], xâƒ—[3]]
    Pâƒ— = SA[Pâƒ—[1], f(xâƒ—[2]), Pâƒ—[2], Pâƒ—[3]] # only Pâƒ—[2] is recalculated
    return LCurveCornerState{T}(xâƒ—, Pâƒ—)
end

function move_right(f::LCurveCornerCachedFunction{T}, state::LCurveCornerState{T}) where {T}
    (; xâƒ—, Pâƒ—) = state
    xâƒ— = SA[xâƒ—[2], xâƒ—[3], xâƒ—[2]+(xâƒ—[4]-xâƒ—[3]), xâƒ—[4]]
    Pâƒ— = SA[Pâƒ—[2], Pâƒ—[3], f(xâƒ—[3]), Pâƒ—[4]] # only Pâƒ—[3] is recalculated
    return LCurveCornerState(xâƒ—, Pâƒ—)
end

function update_curvature!(f::LCurveCornerCachedFunction{T}, state::LCurveCornerState{T}, Pfilter = nothing) where {T}
    (; xâƒ—, Pâƒ—) = state
    for i in 1:4
        x, P, C = xâƒ—[i], Pâƒ—[i], T(-Inf)
        if Pfilter === nothing || Pfilter(P)
            # Compute curvature from nearest neighbours
            xâ‚‹, xâ‚Š = T(-Inf), T(+Inf)
            Pâ‚‹, Pâ‚Š = P, P
            for (_x, (_P, _)) in pairs(f.point_cache)
                (xâ‚‹ < _x < x) && ((xâ‚‹, Pâ‚‹) = (_x, _P))
                (x < _x < xâ‚Š) && ((xâ‚Š, Pâ‚Š) = (_x, _P))
            end
            C = menger(Pâ‚‹, P, Pâ‚Š)
        end
        f.point_cache[x] = LCurveCornerPoint(P, C)
    end
    return state
end

function menger(Pâ±¼::V, Pâ‚–::V, Pâ‚—::V) where {V <: SVector{2}}
    Î”â±¼â‚–, Î”â‚–â‚—, Î”â‚—â±¼ = Pâ±¼ - Pâ‚–, Pâ‚– - Pâ‚—, Pâ‚— - Pâ±¼
    PÌ„â±¼PÌ„â‚–, PÌ„â‚–PÌ„â‚—, PÌ„â‚—PÌ„â±¼ = Î”â±¼â‚– â‹… Î”â±¼â‚–, Î”â‚–â‚— â‹… Î”â‚–â‚—, Î”â‚—â±¼ â‹… Î”â‚—â±¼
    Câ‚– = 2 * (Î”â±¼â‚– Ã— Î”â‚–â‚—) / âˆš(PÌ„â±¼PÌ„â‚– * PÌ„â‚–PÌ„â‚— * PÌ„â‚—PÌ„â±¼)
    return Câ‚–
end

function menger(f; h = 1e-3)
    function menger_curvature_inner(x)
        fâ±¼, fâ‚–, fâ‚— = f(x - h), f(x), f(x + h)
        Pâ±¼, Pâ‚–, Pâ‚— = SA[x-h, fâ±¼], SA[x, fâ‚–], SA[x+h, fâ‚—]
        return menger(Pâ±¼, Pâ‚–, Pâ‚—)
    end
end

function menger(x, y; h = 1e-3)
    function menger_curvature_inner(t)
        xâ‚‹, xâ‚€, xâ‚Š = x(t - h), x(t), x(t + h)
        yâ‚‹, yâ‚€, yâ‚Š = y(t - h), y(t), y(t + h)
        xâ€², xâ€²â€² = (xâ‚Š - xâ‚‹) / 2h, (xâ‚Š - 2xâ‚€ + xâ‚‹) / h^2
        yâ€², yâ€²â€² = (yâ‚Š - yâ‚‹) / 2h, (yâ‚Š - 2yâ‚€ + yâ‚‹) / h^2
        return (xâ€² * yâ€²â€² - yâ€² * xâ€²â€²) / âˆš((xâ€²^2 + yâ€²^2)^3)
    end
end

#=
lin_interp(x, xâ‚, xâ‚‚, yâ‚, yâ‚‚) = yâ‚ + (yâ‚‚ - yâ‚) * (x - xâ‚) / (xâ‚‚ - xâ‚)
exp_interp(x, xâ‚, xâ‚‚, yâ‚, yâ‚‚) = yâ‚ + log1p(expm1(yâ‚‚ - yâ‚) * (x - xâ‚) / (xâ‚‚ - xâ‚))

function menger(x::Dierckx.Spline1D, y::Dierckx.Spline1D)
    function menger_curvature_inner(t)
        xâ€²  = Dierckx.derivative(x, t; nu = 1)
        xâ€²â€² = Dierckx.derivative(x, t; nu = 2)
        yâ€²  = Dierckx.derivative(y, t; nu = 1)
        yâ€²â€² = Dierckx.derivative(y, t; nu = 2)
        return (xâ€² * yâ€²â€² - yâ€² * xâ€²â€²) / âˆš((xâ€²^2 + yâ€²^2)^3)
    end
end

function menger(y::Dierckx.Spline1D)
    function menger_curvature_inner(t)
        yâ€²  = Dierckx.derivative(y, t; nu = 1)
        yâ€²â€² = Dierckx.derivative(y, t; nu = 2)
        return yâ€²â€² / âˆš((1 + yâ€²^2)^3)
    end
end

function menger(xâ±¼::T, xâ‚–::T, xâ‚—::T, Pâ±¼::V, Pâ‚–::V, Pâ‚—::V; interp_uniform = true, linear_deriv = true) where {T, V <: SVector{2, T}}
    if interp_uniform
        Ï† = T(Base.MathConstants.Ï†)
        h = min(abs(xâ‚– - xâ±¼), abs(xâ‚— - xâ‚–)) / Ï†
        hâ‚‹ = hâ‚Š = h
        xâ‚‹, xâ‚€, xâ‚Š = xâ‚– - h, xâ‚–, xâ‚– + h
        Pâ‚€ = Pâ‚–
        Pâ‚‹ = exp_interp.(xâ‚‹, xâ±¼, xâ‚–, Pâ±¼, Pâ‚–)
        Pâ‚Š = exp_interp.(xâ‚Š, xâ‚–, xâ‚—, Pâ‚–, Pâ‚—)
    else
        Pâ‚‹, Pâ‚€, Pâ‚Š = Pâ±¼, Pâ‚–, Pâ‚—
        xâ‚‹, xâ‚€, xâ‚Š = xâ±¼, xâ‚–, xâ‚—
        hâ‚‹, hâ‚Š = xâ‚€ - xâ‚‹, xâ‚Š - xâ‚€
    end
    Î¾â‚‹, Î¾â‚€, Î¾â‚Š = Pâ‚‹[1], Pâ‚€[1], Pâ‚Š[1]
    Î·â‚‹, Î·â‚€, Î·â‚Š = Pâ‚‹[2], Pâ‚€[2], Pâ‚Š[2]

    if linear_deriv
        Î¾â€² = (Î¾â‚Š - Î¾â‚‹) / (hâ‚Š + hâ‚‹)
        Î·â€² = (Î·â‚Š - Î·â‚‹) / (hâ‚Š + hâ‚‹)
    else
        Î¾â€² = (hâ‚‹^2 * Î¾â‚Š + (hâ‚Š + hâ‚‹) * (hâ‚Š - hâ‚‹) * Î¾â‚€ - hâ‚Š^2 * Î¾â‚‹) / (hâ‚Š * hâ‚‹ * (hâ‚Š + hâ‚‹))
        Î·â€² = (hâ‚‹^2 * Î·â‚Š + (hâ‚Š + hâ‚‹) * (hâ‚Š - hâ‚‹) * Î·â‚€ - hâ‚Š^2 * Î·â‚‹) / (hâ‚Š * hâ‚‹ * (hâ‚Š + hâ‚‹))
    end

    Î¾â€²â€² = 2 * (hâ‚‹ * Î¾â‚Š - (hâ‚Š + hâ‚‹) * Î¾â‚€ + hâ‚Š * Î¾â‚‹) / (hâ‚Š * hâ‚‹ * (hâ‚Š + hâ‚‹))
    Î·â€²â€² = 2 * (hâ‚‹ * Î·â‚Š - (hâ‚Š + hâ‚‹) * Î·â‚€ + hâ‚Š * Î·â‚‹) / (hâ‚Š * hâ‚‹ * (hâ‚Š + hâ‚‹))

    return (Î¾â€² * Î·â€²â€² - Î·â€² * Î¾â€²â€²) / âˆš((Î¾â€²^2 + Î·â€²^2)^3)
end

function maximize_curvature(state::LCurveCornerState{T}) where {T}
    # Maximize curvature and transform back from t-space to x-space
    (; xâƒ—, Pâƒ—) = state
    tâ‚, tâ‚‚, tâ‚ƒ, tâ‚„ = T(0), 1 / T(3), 2 / T(3), T(1)
    t_opt, P_opt, C_opt = maximize_curvature(Pâƒ—...)
    x_opt =
        tâ‚ <= t_opt < tâ‚‚ ? lin_interp(t_opt, tâ‚, tâ‚‚, xâƒ—[1], xâƒ—[2]) :
        tâ‚‚ <= t_opt < tâ‚ƒ ? lin_interp(t_opt, tâ‚‚, tâ‚ƒ, xâƒ—[2], xâƒ—[3]) :
        lin_interp(t_opt, tâ‚ƒ, tâ‚„, xâƒ—[3], xâƒ—[4])
    return x_opt
end

function maximize_curvature(Pâ‚::V, Pâ‚‚::V, Pâ‚ƒ::V, Pâ‚„::V; bezier = true) where {T, V <: SVector{2, T}}
    # Analytically maximize curvature of parametric cubic spline fit to data.
    #   see: https://cs.stackexchange.com/a/131032
    a, e = Pâ‚[1], Pâ‚[2]
    b, f = Pâ‚‚[1], Pâ‚‚[2]
    c, g = Pâ‚ƒ[1], Pâ‚ƒ[2]
    d, h = Pâ‚„[1], Pâ‚„[2]

    if bezier
        Î¾ = t -> a * (1 - t)^3 + 3 * b * (1 - t)^2 * t + 3 * c * (1 - t) * t^2 + d * t^3
        Î· = t -> e * (1 - t)^3 + 3 * f * (1 - t)^2 * t + 3 * g * (1 - t) * t^2 + h * t^3
        P = t -> SA{T}[Î¾(t), Î·(t)]

        # In order to keep the equations in a more compact form, introduce the following substitutions:
        m = d - 3 * c + 3 * b - a
        n = c - 2 * b + a
        o = b - a
        p = h - 3 * g + 3 * f - e
        q = g - 2 * f + e
        r = f - e
    else
        Î¾ = t -> a * (t - 1 / 3) * (t - 2 / 3) * (t - 1 / 1) / T((0 / 1 - 1 / 3) * (0 / 1 - 2 / 3) * (0 / 1 - 1 / 1)) + # (-9  * a * t^3)/2  + (+18 * a * t^2)/2 + (-11 * a * t)/2 + a
                 b * (t - 0 / 1) * (t - 2 / 3) * (t - 1 / 1) / T((1 / 3 - 0 / 1) * (1 / 3 - 2 / 3) * (1 / 3 - 1 / 1)) + # (+27 * b * t^3)/2  + (-45 * b * t^2)/2 + (+18 * b * t)/2 +
                 c * (t - 0 / 1) * (t - 1 / 3) * (t - 1 / 1) / T((2 / 3 - 0 / 1) * (2 / 3 - 1 / 3) * (2 / 3 - 1 / 1)) + # (-27 * c * t^3)/2  + (+36 * c * t^2)/2 + ( -9 * c * t)/2 +
                 d * (t - 0 / 1) * (t - 1 / 3) * (t - 2 / 3) / T((1 / 1 - 0 / 1) * (1 / 1 - 1 / 3) * (1 / 1 - 2 / 3))   # (+9  * d * t^3)/2  + ( -9 * d * t^2)/2 + ( +2 * d * t)/2
        Î· = t -> e * (t - 1 / 3) * (t - 2 / 3) * (t - 1 / 1) / T((0 / 1 - 1 / 3) * (0 / 1 - 2 / 3) * (0 / 1 - 1 / 1)) + # (-9  * e * t^3)/2  + (+18 * e * t^2)/2 + (-11 * e * t)/2 + e
                 f * (t - 0 / 1) * (t - 2 / 3) * (t - 1 / 1) / T((1 / 3 - 0 / 1) * (1 / 3 - 2 / 3) * (1 / 3 - 1 / 1)) + # (+27 * f * t^3)/2  + (-45 * f * t^2)/2 + (+18 * f * t)/2 +
                 g * (t - 0 / 1) * (t - 1 / 3) * (t - 1 / 1) / T((2 / 3 - 0 / 1) * (2 / 3 - 1 / 3) * (2 / 3 - 1 / 1)) + # (-27 * g * t^3)/2  + (+36 * g * t^2)/2 + ( -9 * g * t)/2 +
                 h * (t - 0 / 1) * (t - 1 / 3) * (t - 2 / 3) / T((1 / 1 - 0 / 1) * (1 / 1 - 1 / 3) * (1 / 1 - 2 / 3))   # (+9  * h * t^3)/2  + ( -9 * h * t^2)/2 + ( +2 * h * t)/2
        P = t -> SA{T}[Î¾(t), Î·(t)]

        # In order to keep the equations in a more compact form, introduce the following substitutions:
        m = (-9 * a + 27 * b - 27 * c + 9 * d) / 2
        n = (+18 * a - 45 * b + 36 * c - 9 * d) / 6
        o = (-11 * a + 18 * b - 9 * c + 2 * d) / 6
        p = (-9 * e + 27 * f - 27 * g + 9 * h) / 2
        q = (+18 * e - 45 * f + 36 * g - 9 * h) / 6
        r = (-11 * e + 18 * f - 9 * g + 2 * h) / 6
    end

    # This leads to the following simplified derivatives:
    Î¾â€²  = t -> 3 * (m * t^2 + 2 * n * t + o)
    Î¾â€²â€² = t -> 6 * (m * t + n)
    Î·â€²  = t -> 3 * (p * t^2 + 2 * q * t + r)
    Î·â€²â€² = t -> 6 * (p * t + q)

    # Curvature and its derivative:
    k  = t -> ((3 * (m * t^2 + 2 * n * t + o)) * (6 * (p * t + q)) - (3 * (p * t^2 + 2 * q * t + r)) * (6 * (m * t + n))) / ((3 * (m * t^2 + 2 * n * t + o))^2 + (3 * (p * t^2 + 2 * q * t + r))^2)^(3 / 2)
    kâ€² = t -> (-18 * m * (p * t^2 + 2 * q * t + r) + 18 * p * (m * t^2 + 2 * n * t + o) - 18 * (m * t + n) * (2 * p * t + 2 * q) + 18 * (2 * m * t + 2 * n) * (p * t + q)) / (9 * (p * t^2 + 2 * q * t + r)^2 + 9 * (m * t^2 + 2 * n * t + o)^2)^(3 / 2) - (3 * (18 * (p * t + q) * (m * t^2 + 2 * n * t + o) - 18 * (m * t + n) * (p * t^2 + 2 * q * t + r)) * (18 * (2 * p * t + 2 * q) * (p * t^2 + 2 * q * t + r) + 18 * (2 * m * t + 2 * n) * (m * t^2 + 2 * n * t + o))) / (2 * (9 * (p * t^2 + 2 * q * t + r)^2 + 9 * (m * t^2 + 2 * n * t + o)^2)^(5 / 2))

    # Solve analytically
    coeffs = MVector{6, Complex{T}}(
        (1296 * m * p^2 + 1296 * m^3) * q - 1296 * n * p^3 - 1296 * m^2 * n * p,
        (1620 * m * p^2 + 1620 * m^3) * r + 3240 * m * p * q^2 + (3240 * m^2 * n - 3240 * n * p^2) * q - 1620 * o * p^3 + ((-1620 * m^2 * o) - 3240 * m * n^2) * p,
        (5184 * m * p * q + 1296 * n * p^2 + 6480 * m^2 * n) * r + 1296 * m * q^3 - 1296 * n * p * q^2 + ((-6480 * o * p^2) - 1296 * m^2 * o + 1296 * m * n^2) * q + ((-5184 * m * n * o) - 1296 * n^3) * p,
        1296 * m * p * r^2 + (1944 * m * q^2 + 6480 * n * p * q - 1296 * o * p^2 + 1296 * m^2 * o + 8424 * m * n^2) * r - 8424 * o * p * q^2 - 6480 * m * n * o * q + ((-1296 * m * o^2) - 1944 * n^2 * o) * p,
        2592 * n * p * r^2 + (3888 * n * q^2 - 2592 * o * p * q + 2592 * m * n * o + 3888 * n^3) * r - 3888 * o * q^3 + ((-2592 * m * o^2) - 3888 * n^2 * o) * q,
        -324 * m * r^3 + (1944 * n * q + 324 * o * p) * r^2 + ((-1944 * o * q^2) - 324 * m * o^2 + 1944 * n^2 * o) * r - 1944 * n * o^2 * q + 324 * o^3 * p,
    )
    roots = MVector{6, Complex{T}}(undef)
    PolynomialRoots.roots5!(roots, coeffs, eps(T), true)

    # Check roots and return maximum
    tâ‚, tâ‚„ = zero(T), one(T)
    kâ‚, kâ‚„ = k(tâ‚), k(tâ‚„)
    tmax, Pmax, kmax = kâ‚ > kâ‚„ ? (tâ‚, Pâ‚, kâ‚) : (tâ‚„, Pâ‚„, kâ‚„)
    for ráµ¢ in roots
        _t, _s = real(ráµ¢), imag(ráµ¢)
        !(_s â‰ˆ 0) && continue # real roots only
        !(tâ‚ <= _t <= tâ‚„) && continue # filter roots within range
        ((_k = k(_t)) > kmax) && ((tmax, Pmax, kmax) = (_t, P(_t), _k))
    end

    return tmax, Pmax, kmax
end

function directed_angle(vâ‚::V, vâ‚‚::V) where {T, V <: SVector{2, T}}
    Î± = atan(vâ‚[2], vâ‚[1]) - atan(vâ‚‚[2], vâ‚‚[1])
    return Î± < 0 ? 2 * T(Ï€) + Î± : Î±
end
directed_angle(Pâ±¼::V, Pâ‚–::V, Pâ‚—::V) where {V <: SVector{2}} = directed_angle(Pâ±¼ - Pâ‚–, Pâ‚— - Pâ‚–)

function kahan_angle(vâ‚::V, vâ‚‚::V) where {T, V <: SVector{2, T}}
    # Kahan's method for computing the angle between vâ‚ and vâ‚‚.
    #   see: https://scicomp.stackexchange.com/a/27694
    a, b, c = norm(vâ‚), norm(vâ‚‚), norm(vâ‚ - vâ‚‚)
    a, b = max(a, b), min(a, b)
    Î¼ = b â‰¥ c ? c - (a - b) : (b - (a - c))
    num = ((a - b) + c) * max(Î¼, zero(T))
    den = (a + (b + c)) * ((a - c) + b)
    Î± = 2 * atan(âˆš(num / den))
    return vâ‚ Ã— vâ‚‚ > 0 ? 2 * T(Ï€) - Î± : Î±
end
kahan_angle(Pâ±¼::V, Pâ‚–::V, Pâ‚—::V) where {V <: SVector{2}} = kahan_angle(Pâ±¼ - Pâ‚–, Pâ‚— - Pâ‚–)
=#

####
#### GCV method for choosing the Tikhonov regularization parameter
####

struct NNLSGCVRegProblem{T, TA <: AbstractMatrix{T}, Tb <: AbstractVector{T}, W0, W1, W2}
    A::TA
    b::Tb
    m::Int
    n::Int
    Î³::Vector{T}
    svd_work::W0
    nnls_prob::W1
    nnls_prob_smooth_cache::W2
end
function NNLSGCVRegProblem(A::AbstractMatrix{T}, b::AbstractVector{T}) where {T}
    m, n = size(A)
    svd_work = SVDValsWorkspace(A) # workspace for computing singular values
    nnls_prob = NNLSProblem(A, b)
    nnls_prob_smooth_cache = NNLSTikhonovRegProblemCache(A, b)
    Î³ = svd_work.S # store reference to (generalized) singular values for convenience
    return NNLSGCVRegProblem(A, b, m, n, Î³, svd_work, nnls_prob, nnls_prob_smooth_cache)
end

@inline solution(work::NNLSGCVRegProblem) = solution(work.nnls_prob_smooth_cache[])
@inline ncomponents(work::NNLSGCVRegProblem) = ncomponents(work.nnls_prob_smooth_cache[])
@inline LinearAlgebra.svdvals!(work::NNLSGCVRegProblem, A = work.A) = svdvals!(work.svd_work, A)

@doc raw"""
    lsqnonneg_gcv(A::AbstractMatrix, b::AbstractVector)

Compute the Tikhonov-regularized nonnegative least-squares (NNLS) solution ``X_{\mu}`` of the problem:

```math
X_{\mu} = \underset{x \ge 0}{\operatorname{argmin}}\; ||Ax - b||_2^2 + \mu^2 ||L x||_2^2
```

where ``L`` is the identity matrix, and ``\mu`` is chosen via the Generalized Cross-Validation (GCV) method:

```math
\mu = \underset{\nu \ge 0}{\operatorname{argmin}}\; \frac{||AX_{\nu} - b||_2^2}{\mathcal{T}(\nu)^2}
```

where ``\mathcal{T}(\mu)`` is the "degrees of freedom" of the regularized system

```math
\mathcal{T}(\mu) = \operatorname{tr}(I - A (A^T A + \mu^2 L^T L) A^T).
```

Details of the GCV method can be found in Hansen (1992)[1].

# Arguments

  - `A::AbstractMatrix`: Decay basis matrix
  - `b::AbstractVector`: Decay curve data

# Outputs

  - `X::AbstractVector`: Regularized NNLS solution
  - `mu::Real`: Resulting regularization parameter ``\mu``
  - `chi2::Real`: Resulting increase in residual norm relative to the unregularized ``\mu = 0`` solution

# References

  1. Hansen, P.C., 1992. Analysis of Discrete Ill-Posed Problems by Means of the L-Curve. SIAM Review, 34(4), 561-580, https://doi.org/10.1137/1034115.
"""
function lsqnonneg_gcv(A::AbstractMatrix, b::AbstractVector; kwargs...)
    work = lsqnonneg_gcv_work(A, b)
    return lsqnonneg_gcv!(work; kwargs...)
end
lsqnonneg_gcv_work(A::AbstractMatrix, b::AbstractVector) = NNLSGCVRegProblem(A, b)

function lsqnonneg_gcv!(work::NNLSGCVRegProblem{T}; method = :brent, init = -4.0, bounds = (-8.0, 2.0), rtol = 0.05, atol = 1e-4, maxiters = 10) where {T}
    # Find Î¼ by minimizing the function G(Î¼) (GCV method)
    @assert bounds[1] < init < bounds[2] "Initial value must be within bounds"
    logÎ¼â‚‹, logÎ¼â‚Š = T.(bounds)
    logÎ¼â‚€ = T(init)

    # Precompute singular values for GCV computation
    svdvals!(work)

    # Non-zero lower bound for GCV to avoid log(0) in the objective function
    gcv_low = gcv_lower_bound(work)

    # Objective functions
    reset_cache!(work.nnls_prob_smooth_cache)
    function logğ’¢(logÎ¼)
        ğ’¢ = gcv!(work, logÎ¼)
        ğ’¢ = max(ğ’¢, gcv_low)
        return log(ğ’¢)
    end
    function logğ’¢_and_âˆ‡logğ’¢(logÎ¼)
        ğ’¢, âˆ‡ğ’¢ = gcv_and_âˆ‡gcv!(work, logÎ¼)
        ğ’¢ = max(ğ’¢, gcv_low)
        return log(ğ’¢), âˆ‡ğ’¢ / ğ’¢
    end

    if method === :nlopt
        # alg = :LN_COBYLA # local, gradient-free, linear approximation of objective
        alg = :LN_BOBYQA # local, gradient-free, quadratic approximation of objective
        # alg = :GN_AGS # global, gradient-free, hilbert curve based dimension reduction
        # alg = :LN_NELDERMEAD # local, gradient-free, simplex method
        # alg = :LN_SBPLX # local, gradient-free, subspace searching simplex method
        # alg = :LD_CCSAQ # local, first-order (rough ranking: [:LD_MMA, :LD_SLSQP, :LD_LBFGS, :LD_CCSAQ, :LD_AUGLAG])
        opt               = NLopt.Opt(alg, 1)
        opt.lower_bounds  = Float64(logÎ¼â‚‹)
        opt.upper_bounds  = Float64(logÎ¼â‚Š)
        opt.xtol_abs      = Float64(atol)
        opt.xtol_rel      = Float64(rtol)
        opt.ftol_abs      = 0.0
        opt.ftol_rel      = 0.0
        opt.min_objective = (logÎ¼, âˆ‡logÎ¼) -> @inbounds Float64(logğ’¢(T(logÎ¼[1])))
        minf, minx, ret   = NLopt.optimize(opt, Float64[logÎ¼â‚€])
        logmu_final       = @inbounds T(minx[1])
        logğ’¢_final        = T(minf)
    elseif method === :brent
        logmu_final, logğ’¢_final = brent_minimize(logğ’¢, logÎ¼â‚‹, logÎ¼â‚Š; xrtol = T(rtol), xatol = T(atol), maxiters)
    elseif method === :brent_newton
        logğ’¢â‚‹, âˆ‡logğ’¢â‚‹ = logğ’¢_and_âˆ‡logğ’¢(logÎ¼â‚‹)
        logğ’¢â‚Š, âˆ‡logğ’¢â‚Š = logğ’¢_and_âˆ‡logğ’¢(logÎ¼â‚Š)
        logÎ¼_bdry, logğ’¢_bdry = logğ’¢â‚‹ < logğ’¢â‚Š ? (logÎ¼â‚‹, logğ’¢â‚‹) : (logÎ¼â‚Š, logğ’¢â‚Š)
        if âˆ‡logğ’¢â‚‹ < 0 && âˆ‡logğ’¢â‚Š > 0
            logğ’¢â‚€, âˆ‡logğ’¢â‚€ = logğ’¢_and_âˆ‡logğ’¢(logÎ¼â‚€)
            logmu_final, logğ’¢_final = brent_newton_minimize(logğ’¢_and_âˆ‡logğ’¢, logÎ¼â‚‹, logÎ¼â‚Š, logÎ¼â‚€, logğ’¢â‚€, âˆ‡logğ’¢â‚€; xrtol = T(rtol), xatol = T(atol), maxiters)
        else
            logmu_final, logğ’¢_final = logÎ¼_bdry, logğ’¢_bdry
        end
        if logğ’¢_bdry < logğ’¢_final
            logmu_final, logğ’¢_final = logÎ¼_bdry, logğ’¢_bdry
        end
    else
        error("Unknown minimization method: $method")
    end

    # Return the final regularized solution
    mu_final = exp(logmu_final)
    x_final = solve!(work.nnls_prob_smooth_cache, mu_final)
    x_unreg = solve!(work.nnls_prob)
    chi2_final = resnorm_sq(work.nnls_prob_smooth_cache[]) / resnorm_sq(work.nnls_prob)

    return (; x = x_final, mu = mu_final, chi2 = chi2_final)
end

# Implements equation (32) from:
#
#   Analysis of Discrete Ill-Posed Problems by Means of the L-Curve
#   Hansen et al. 1992 (https://epubs.siam.org/doi/10.1137/1034115)
#
# where here L = Id and Î» = Î¼.
function gcv!(work::NNLSGCVRegProblem, logÎ¼)
    # Unpack buffers
    #   NOTE: assumes `svdvals!(work)` has been called and that the singular values `work.Î³` are ready
    (; m, n, Î³) = work

    # Solve regularized NNLS problem
    Î¼ = exp(logÎ¼)
    solve!(work.nnls_prob_smooth_cache, Î¼)
    cache = work.nnls_prob_smooth_cache[]

    # Compute GCV
    resÂ² = resnorm_sq(cache) # squared residual norm ||A * x(Î¼) - b||^2
    dof = gcv_dof(m, n, Î³, Î¼) # degrees of freedom; Î³ are (generalized) singular values
    gcv = resÂ² / dof^2

    return gcv
end

function gcv_and_âˆ‡gcv!(work::NNLSGCVRegProblem, logÎ¼)
    # Unpack buffers
    #   NOTE: assumes `svdvals!(work)` has been called and that the singular values `work.Î³` are ready
    (; m, n, Î³) = work

    # Solve regularized NNLS problem
    Î¼ = exp(logÎ¼)
    solve!(work.nnls_prob_smooth_cache, Î¼)
    cache = work.nnls_prob_smooth_cache[]

    # Compute primal
    resÂ² = resnorm_sq(cache) # squared residual norm ||A * x(Î¼) - b||^2
    dof = gcv_dof(m, n, Î³, Î¼) # degrees of freedom; Î³ are (generalized) singular values
    gcv = resÂ² / dof^2

    # Compute derivative: âˆ‚/âˆ‚Î» [resnorm_sq(Î») / dof(Î»)^2] = âˆ‡resnorm_sq(Î») / dof(Î»)^2 - 2 * resnorm_sq(Î») * âˆ‡dof(Î») / dof(Î»)^3
    âˆ‡resÂ² = âˆ‡resnorm_sq(cache)
    âˆ‡dof = âˆ‡gcv_dof(m, n, Î³, Î¼)
    âˆ‡gcv = (âˆ‡resÂ² - 2 * resÂ² * âˆ‡dof / dof) / dof^2

    return gcv, âˆ‡gcv
end

# Non-trivial lower bound of the GCV function
#   GCV(Î¼) = ||A * x(Î¼) - b||^2 / ğ’¯(Î¼)^2
# where ğ’¯(Î¼) is the "degrees of freedom" of the regularized system
#   ğ’¯(Î¼) = tr(I - A * (A'A + Î¼Â²I)â»Â¹ * A')
#        âˆˆ [max(m - n, 0), m)
# The trivial lower bound GCV(Î¼) = 0 can (sometimes) be achieved when Î¼ = 0 if ||A * x(Î¼ = 0) - b|| = 0.
# Let Îµ > 0 be the RMSE threshold below which we consider the solution exact, i.e. bound ||A * x(Î¼) - b|| / âˆšm >= Îµ.
# Then, GCV(Î¼) = ||A * x(Î¼) - b||^2 / ğ’¯(Î¼)^2 >= (âˆšm * Îµ)^2 / m^2 = Îµ^2 / m
gcv_lower_bound(m::Int, n::Int, Îµ::Real) = Îµ^2 / m
gcv_lower_bound(work::NNLSGCVRegProblem{T}, Îµ::T = eps(T)) where {T} = gcv_lower_bound(work.m, work.n, Îµ)

#=
# Equivalent direct method (less efficient)
function gcv!(work::NNLSGCVRegProblem, logÎ¼, ::Val{extract_subproblem} = Val(false)) where {extract_subproblem}
    # Unpack buffers
    (; A, b, m, n, AÎ¼, A_buf, Aáµ€_buf, Aáµ€A_buf) = work

    # Solve regularized NNLS problem and record residual norm ||A * x(Î¼) - b||^2
    Î¼ = exp(logÎ¼)
    solve!(work.nnls_prob_smooth_cache, Î¼)
    resÂ² = resnorm_sq(work.nnls_prob_smooth_cache[])

    if extract_subproblem
        # Extract equivalent unconstrained least squares subproblem from NNLS problem
        # by extracting columns of A which correspond to nonzero components of x(Î¼)
        idx = NNLS.components(work.nnls_prob_smooth_cache[].nnls_prob.nnls_work)
        nâ€² = length(idx)
        Aâ€² = reshape(view(A_buf, 1:m*nâ€²), m, nâ€²)
        Atâ€² = reshape(view(Aáµ€_buf, 1:nâ€²*m), nâ€², m)
        AtAâ€² = reshape(view(Aáµ€A_buf, 1:nâ€²*nâ€²), nâ€², nâ€²)
        copyto!(Aâ€², view(A, :, idx))
    else
        # Use full matrix
        Aâ€² = A
        Atâ€² = Aáµ€_buf
        AtAâ€² = Aáµ€A_buf
    end

    # Efficient compution of
    #   AÎ¼ = A * (A'A + Î¼Â²I)â»Â¹ * A'
    # where the matrices have sizes
    #   A: (m, n), AÎ¼: (m, m), At: (n, m), AtA: (n, n)
    mul!(AtAâ€², Aâ€²', Aâ€²) # A'A
    @simd for i in 1:n
        AtAâ€²[i, i] += Î¼^2 # A'A + Î¼Â²I
    end
    ldiv!(Atâ€², cholesky!(Symmetric(AtAâ€²)), Aâ€²') # (A'A + Î¼Â²I)â»Â¹ * A'
    mul!(AÎ¼, Aâ€², Atâ€²) # A * (A'A + Î¼Â²I)â»Â¹ * A'

    # Return Generalized cross-validation. See equations 27 and 32 in
    #   Hansen, P.C., 1992. Analysis of Discrete Ill-Posed Problems by Means of the L-Curve. SIAM Review, 34(4), 561-580
    #   https://doi.org/10.1137/1034115
    dof = m - tr(AÎ¼) # tr(I - AÎ¼) = m - tr(AÎ¼) for m x m matrix AÎ¼; can be considered as the "degrees of freedom" (Hansen, 1992)
    gcv = resÂ² / dof^2 # ||A * x(Î¼) - b||^2 / tr(I - AÎ¼)^2

    return gcv
end
=#

# Equation (27) from Hansen et al. 1992 (https://epubs.siam.org/doi/10.1137/1034115),
# specialized for L = identity:
#
#   tr(I_m - A * (A'A + Î»^2 * L'L)â»Â¹ * A') = m - n + sum_i Î»^2 / (Î³_i^2 + Î»^2)
#
# where Î³_i are the generalized singular values, which are equivalent to ordinary
# singular values when L = identity, and size(A) = (m, n).
# Can be considered as the "degrees of freedom".
function gcv_dof(m::Int, n::Int, Î³::AbstractVector{T}, Î»::T) where {T}
    dof = T(max(m - n, 0)) # handle underdetermined systems (m < n)
    Î»Â² = abs2(Î»)
    @simd for Î³áµ¢ in Î³
        Î³áµ¢Â² = abs2(Î³áµ¢)
        dof += Î»Â² / (Î³áµ¢Â² + Î»Â²)
    end
    return dof
end
gcv_dof(A::AbstractMatrix{T}, Î»::T) where {T} = gcv_dof(size(A)..., svdvals(A), Î»)

# DOF derivative: âˆ‚/âˆ‚Î» gcv_dof(m, n, Î³, Î»)
function âˆ‡gcv_dof(m::Int, n::Int, Î³::AbstractVector{T}, Î»::T) where {T}
    âˆ‡dof = zero(T)
    Î»Â² = abs2(Î»)
    @simd for Î³áµ¢ in Î³
        Î³áµ¢Â² = abs2(Î³áµ¢)
        âˆ‡dof += 2 * Î» * Î³áµ¢Â² / (Î³áµ¢Â² + Î»Â²)^2
    end
    return âˆ‡dof
end
âˆ‡gcv_dof(A::AbstractMatrix{T}, Î»::T) where {T} = âˆ‡gcv_dof(size(A)..., svdvals(A), Î»)
